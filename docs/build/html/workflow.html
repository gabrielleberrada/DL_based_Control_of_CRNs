<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Workflow &mdash; Computing the Fisher Information of CRNs using Deep Learning 0.1 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/sphinx_highlight.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Examples of Chemical Reaction Networks" href="demos.html" />
    <link rel="prev" title="Efficient Fisher Information Computation and Policy Srearch in Sampled Stochastic Chemical Reaction Networks through Deep Learning" href="index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> Computing the Fisher Information of CRNs using Deep Learning
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Workflow</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#step-1-specify-the-chemical-reaction-network-to-work-on">Step 1: Specify the Chemical Reaction Network to work on</a></li>
<li class="toctree-l2"><a class="reference internal" href="#step-2-generate-the-datasets">Step 2: Generate the Datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="#step-3-import-data">Step 3: Import data</a></li>
<li class="toctree-l2"><a class="reference internal" href="#step-4-train-the-mixture-density-network">Step 4: Train the Mixture Density Network</a></li>
<li class="toctree-l2"><a class="reference internal" href="#step-5-estimate-the-probability-mass-functions-and-sensitivities-of-the-likelihood">Step 5: Estimate the probability mass functions and sensitivities of the likelihood</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#predict-probability-mass-function">Predict probability mass function</a></li>
<li class="toctree-l3"><a class="reference internal" href="#predict-the-sensitivity-of-the-likelihood">Predict the sensitivity of the likelihood</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#step-6a-estimate-the-fisher-information">Step 6A: Estimate the Fisher Information</a></li>
<li class="toctree-l2"><a class="reference internal" href="#step-6b-estimate-the-expectation-and-its-gradient">Step 6B: Estimate the expectation and its gradient</a></li>
<li class="toctree-l2"><a class="reference internal" href="#step-7-find-the-optimal-control-parameters">Step 7: Find the optimal control parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="#optional-steps">Optional steps</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#compare-with-finite-state-projection-results">Compare with Finite State Projection results</a></li>
<li class="toctree-l3"><a class="reference internal" href="#use-regularisation-methods">Use regularisation methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="#tune-hyperparameters">Tune hyperparameters</a></li>
<li class="toctree-l3"><a class="reference internal" href="#plot-probability-distributions-and-sensitivities-distributions">Plot probability distributions and sensitivities distributions</a></li>
<li class="toctree-l3"><a class="reference internal" href="#plot-the-fisher-information-values-in-a-table-or-in-barplots">Plot the Fisher Information values in a table or in barplots</a></li>
<li class="toctree-l3"><a class="reference internal" href="#plot-the-expectation-and-its-gradient-in-a-table-or-in-barplots">Plot the expectation and its gradient in a table or in barplots</a></li>
<li class="toctree-l3"><a class="reference internal" href="#store-and-load-mixture-density-networks-weights">Store and load Mixture Density Networks weights</a></li>
<li class="toctree-l3"><a class="reference internal" href="#estimate-the-loss-of-the-projected-gradient-descent-based-on-stochastic-simulations">Estimate the loss of the Projected Gradient Descent based on Stochastic Simulations</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="demos.html">Examples of Chemical Reaction Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="advice.html">Some advice on the implementation of the approach</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="usage.html">API Documentation</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="neuralnetwork.html">Background on the Architecture of the Mixture Density Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="control.html">Background on the Stochastic control of Chemical Reaction Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="math.html">Background on the Fisher Information</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="sources.html">Sources</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Computing the Fisher Information of CRNs using Deep Learning</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Workflow</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/workflow.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="workflow">
<h1>Workflow<a class="headerlink" href="#workflow" title="Permalink to this heading">ÔÉÅ</a></h1>
<p>The complete, step-by-step approach to compute the Fisher Information for Chemical Reaction Networks and Optimal Control experiments
using Mixture Density Networks is introduced below.</p>
<p>Some useful notations are gathered in the following table:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 76%" />
<col style="width: 24%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p>Denomination</p></td>
<td><p>Notation</p></td>
</tr>
<tr class="row-even"><td><p>Fisher Information at time <span class="math notranslate nohighlight">\(t\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(I_t^{\theta,\xi}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Number of control parameters of the propensity functions <span class="math notranslate nohighlight">\(\xi\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(M_{\xi}\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Number of elements in the case of a finite state-space</p></td>
<td><p><span class="math notranslate nohighlight">\(N_{\max}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Number of fixed parameters of the propensity functions <span class="math notranslate nohighlight">\(\theta\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(M_{\theta}\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Number of reactions</p></td>
<td><p><span class="math notranslate nohighlight">\(M\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Number of species</p></td>
<td><p><span class="math notranslate nohighlight">\(N\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Number of time windows</p></td>
<td><p><span class="math notranslate nohighlight">\(L\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Probability mass function evaluated at the <span class="math notranslate nohighlight">\(\ell\)</span>-th element of the enumerated state-space</p></td>
<td><p><span class="math notranslate nohighlight">\(p_\ell(t, \theta, \xi)\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Sensitivity of the likelihood matrix</p></td>
<td><p><span class="math notranslate nohighlight">\(S_t^{\theta,\xi}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Total number of parameters of the propensity functions <span class="math notranslate nohighlight">\(M_{\theta} + M_{\xi}\times L\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(M_{\text{tot}}\)</span></p></td>
</tr>
</tbody>
</table>
<section id="step-1-specify-the-chemical-reaction-network-to-work-on">
<h2>Step 1: Specify the Chemical Reaction Network to work on<a class="headerlink" href="#step-1-specify-the-chemical-reaction-network-to-work-on" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>To fully define the Chemical Reaction Network to work on, define:</p>
<ol class="arabic simple">
<li><p>The stoichiometry matrix. It should have shape <span class="math notranslate nohighlight">\((N, M)\)</span>.</p></li>
<li><p>The propensity functions. Their first argument is a list of parameters, their second argument is the state.
For instance, for a Production and Degradation Chemical Reaction Network:</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">propensity_production</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">propensity_degradation</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>The initial state.</p></li>
<li><p>The number of fixed parameters <span class="math notranslate nohighlight">\(M_{\theta}\)</span> and the number of control parameters <span class="math notranslate nohighlight">\(M_{\xi}\)</span>.</p></li>
<li><p>The index of the species to study (only one).</p></li>
</ol>
<p>The Chemical Reaction Network is then completely defined.</p>
<p>If it does not follow mass-action kinetics, it is also necessary to specify the propensity derivatives with respect to each parameter for the Finite State Projection method.
These derivatives should be stored in an array of shape <span class="math notranslate nohighlight">\((M, M_{\theta}+M_{\xi})\)</span>.</p>
<p>In the demos of the repository, the information for the Chemical Reaction Networks used as examples are stored in files called <code class="xref py py-meth docutils literal notranslate"><span class="pre">propensities_[CRN_name].py()</span></code>.</p>
</section>
<section id="step-2-generate-the-datasets">
<h2>Step 2: Generate the Datasets<a class="headerlink" href="#step-2-generate-the-datasets" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>Datasets <span class="math notranslate nohighlight">\(\mathcal{D}_t^{\theta,\xi}\)</span> are generated from Stochastic Simulations, using the Stochastic Simulation Algorithm as outlined in <span id="id1">[<a class="reference internal" href="sources.html#id7" title="Daniel T Gillespie. A general method for numerically simulating the stochastic time evolution of coupled chemical reactions. Journal of Computational Physics., 22:403‚Äì434, 1976.">Gil76</a>]</span>.
However other simulation algorithms can be implemented.
The simulations are generated using Sobol sequences to choose the parameters over specified intervals.</p>
<p>To perform a single simulation, run the <a class="reference external" href="https://github.com/gabrielleberrada/DL_based_Control_of_CRNs/blob/main/simulation.py">simulation.py</a> file.
To generate multiple simulations and estimate mass functions, use the <a class="reference external" href="https://github.com/gabrielleberrada/DL_based_Control_of_CRNs/blob/main/generate_data.py">generate_data.py</a> file.
The method <a class="reference internal" href="usage.html#generate_csv.generate_csv_datasets" title="generate_csv.generate_csv_datasets"><code class="xref py py-meth docutils literal notranslate"><span class="pre">generate_csv.generate_csv_datasets()</span></code></a> in the <a class="reference external" href="https://github.com/gabrielleberrada/DL_based_Control_of_CRNs/blob/main/generate_csv.py">generate_csv.py</a> can be used to generate multiple datasets at once and store them in CSV files.</p>
<p>These datasets can be used for training, validation or testing purposes.
When training one or multiple Mixture Density Networks for a single Chemical Reaction Network,
it is recommended to generate all data at once and then split the initial dataset into smaller ones
to avoid overlap in the various training, validation and testing datasets.</p>
</section>
<section id="step-3-import-data">
<h2>Step 3: Import data<a class="headerlink" href="#step-3-import-data" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>To load the datasets saved in CSV files as tensors (PyTorch) or arrays (NumPy), use the file <a class="reference external" href="https://github.com/gabrielleberrada/DL_based_control_of_CRNs/blob/main/convert_csv.py">convert_csv.py</a>.</p>
<p>Here is an example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">FILE_NAME</span> <span class="o">=</span> <span class="s1">&#39;file_name&#39;</span>
<span class="n">CRN_NAME</span> <span class="o">=</span> <span class="s1">&#39;crn_name&#39;</span>
<span class="n">NUM_PARAMS</span> <span class="o">=</span> <span class="n">num_params</span>

<span class="c1"># loading data</span>
<span class="n">X_train1</span> <span class="o">=</span> <span class="n">convert_csv</span><span class="o">.</span><span class="n">csv_to_tensor</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">FILE_NAME</span><span class="si">}</span><span class="s1">/X_</span><span class="si">{</span><span class="n">CRN_NAME</span><span class="si">}</span><span class="s1">_train.csv&#39;</span><span class="p">)</span>
<span class="n">y_train1</span> <span class="o">=</span> <span class="n">convert_csv</span><span class="o">.</span><span class="n">csv_to_tensor</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">FILE_NAME</span><span class="si">}</span><span class="s1">/y_</span><span class="si">{</span><span class="n">CRN_NAME</span><span class="si">}</span><span class="s1">_train.csv&#39;</span><span class="p">)</span>

<span class="n">X_valid1</span> <span class="o">=</span> <span class="n">convert_csv</span><span class="o">.</span><span class="n">csv_to_tensor</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">FILE_NAME</span><span class="si">}</span><span class="s1">/X_</span><span class="si">{</span><span class="n">CRN_NAME</span><span class="si">}</span><span class="s1">_valid.csv&#39;</span><span class="p">)</span>
<span class="n">y_valid1</span> <span class="o">=</span> <span class="n">convert_csv</span><span class="o">.</span><span class="n">csv_to_tensor</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">FILE_NAME</span><span class="si">}</span><span class="s1">/y_</span><span class="si">{</span><span class="n">CRN_NAME</span><span class="si">}</span><span class="s1">_valid.csv&#39;</span><span class="p">)</span>

<span class="n">X_test</span> <span class="o">=</span> <span class="n">convert_csv</span><span class="o">.</span><span class="n">csv_to_tensor</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">FILE_NAME</span><span class="si">}</span><span class="s1">/X_</span><span class="si">{</span><span class="n">CRN_NAME</span><span class="si">}</span><span class="s1">_test.csv&#39;</span><span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">convert_csv</span><span class="o">.</span><span class="n">csv_to_tensor</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">FILE_NAME</span><span class="si">}</span><span class="s1">/y_</span><span class="si">{</span><span class="n">CRN_NAME</span><span class="si">}</span><span class="s1">_test.csv&#39;</span><span class="p">)</span>

<span class="n">train_data</span> <span class="o">=</span> <span class="p">[</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">]</span>
<span class="n">valid_data</span> <span class="o">=</span> <span class="p">[</span><span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">]</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="p">[</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section id="step-4-train-the-mixture-density-network">
<h2>Step 4: Train the Mixture Density Network<a class="headerlink" href="#step-4-train-the-mixture-density-network" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>Once the hyperparameters are defined, a Mixture Density Network can be trained on the simulated data:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">neuralnetwork</span><span class="o">.</span><span class="n">NeuralNetwork</span><span class="p">(</span><span class="n">n_comps</span><span class="o">=</span><span class="n">N_COMPS</span><span class="p">,</span> <span class="n">n_params</span><span class="o">=</span><span class="n">NUM_PARAMS</span><span class="p">,</span> <span class="n">n_hidden</span><span class="o">=</span><span class="n">N_HIDDEN</span><span class="p">,</span> <span class="n">mixture</span><span class="o">=</span><span class="n">mixture</span><span class="p">)</span>
<span class="n">train_losses</span><span class="p">,</span> <span class="n">valid_losses</span> <span class="o">=</span> <span class="n">neuralnetwork</span><span class="o">.</span><span class="n">train_NN</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_data</span><span class="p">,</span> <span class="n">valid_data</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">neuralnetwork</span><span class="o">.</span><span class="n">loss_kldivergence</span><span class="p">,</span> <span class="n">max_rounds</span><span class="o">=</span><span class="n">N_ITER</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">LR</span><span class="p">,</span> <span class="n">batchsize</span><span class="o">=</span><span class="n">BATCHSIZE</span><span class="p">)</span>
</pre></div>
</div>
<p>To print the computed losses:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training dataset&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;KLD : </span><span class="si">{</span><span class="n">neuralnetwork</span><span class="o">.</span><span class="n">mean_loss</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">neuralnetwork</span><span class="o">.</span><span class="n">loss_kldivergence</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Hellinger : </span><span class="si">{</span><span class="n">neuralnetwork</span><span class="o">.</span><span class="n">mean_loss</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">neuralnetwork</span><span class="o">.</span><span class="n">loss_hellinger</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Test dataset&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;KLD : </span><span class="si">{</span><span class="n">neuralnetwork</span><span class="o">.</span><span class="n">mean_loss</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">neuralnetwork</span><span class="o">.</span><span class="n">loss_kldivergence</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Hellinger : </span><span class="si">{</span><span class="n">neuralnetwork</span><span class="o">.</span><span class="n">mean_loss</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">neuralnetwork</span><span class="o">.</span><span class="n">loss_hellinger</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that the validation dataset is not involved in the training process. However, if you use an early stopping method, the training process will stop based on the validation loss.</p>
<p>The file <a class="reference external" href="https://github.com/gabrielleberrada/DL_based_Control_of_CRNs/blob/main/training.py">training.py</a> provides an example of the code implementation used for training and evaluating the model‚Äôs performance with loss calculation.</p>
</section>
<section id="step-5-estimate-the-probability-mass-functions-and-sensitivities-of-the-likelihood">
<h2>Step 5: Estimate the probability mass functions and sensitivities of the likelihood<a class="headerlink" href="#step-5-estimate-the-probability-mass-functions-and-sensitivities-of-the-likelihood" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>A trained Mixture Density Network can predict probability distributions.</p>
<ul class="simple">
<li><p>Inputs of the Mixture Density Network: <span class="math notranslate nohighlight">\([t, \theta_1, ..., \theta_{M_{\theta}}, \xi_1^1, \xi_1^2, ..., \xi_1^{M_{\xi}}, \xi_2^1, ..., \xi_L^{M_{\xi}}]\)</span> of type <code class="docutils literal notranslate"><span class="pre">torch.tensor</span></code>.</p></li>
<li><p>Outputs of the Mixture Density Network: The mixture parameters <span class="math notranslate nohighlight">\([w, r, q]\)</span> where <span class="math notranslate nohighlight">\(w\)</span> are the mixture weights, <span class="math notranslate nohighlight">\(r\)</span> and <span class="math notranslate nohighlight">\(q\)</span> are the Negative Binomials parameters
(numbers of successes before the experiment is stopped and probabilities of success for each experiment). In case of a Poisson Mixture, it will only return parameters <span class="math notranslate nohighlight">\([w, r]\)</span>.
The outputs are of type <code class="docutils literal notranslate"><span class="pre">torch.tensor</span></code> and are linked to the computational graph.</p></li>
</ul>
<section id="predict-probability-mass-function">
<h3>Predict probability mass function<a class="headerlink" href="#predict-probability-mass-function" title="Permalink to this heading">ÔÉÅ</a></h3>
<p>To predict a probability mass function, use the function <a class="reference internal" href="usage.html#get_sensitivities.probabilities" title="get_sensitivities.probabilities"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get_sensitivities.probabilities()</span></code></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">up_bound</span> <span class="o">=</span> <span class="mi">500</span> <span class="c1"># to choose the upper boundary of the predicted distribution</span>
<span class="n">to_pred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">t</span><span class="p">,</span> <span class="n">theta_1</span><span class="p">,</span> <span class="o">..</span><span class="p">,</span> <span class="n">theta_M_theta</span><span class="p">,</span> <span class="n">xi_1_1</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="n">xi_L_</span><span class="p">{</span><span class="n">M_xi</span><span class="p">}])</span>

<span class="n">y_pred</span> <span class="o">=</span> <span class="n">get_sensitivities</span><span class="o">.</span><span class="n">probabilities</span><span class="p">(</span><span class="n">to_pred</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">up_bound</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">y_pred</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="c1"># to get the prediction as a NumPy array</span>
</pre></div>
</div>
</section>
<section id="predict-the-sensitivity-of-the-likelihood">
<h3>Predict the sensitivity of the likelihood<a class="headerlink" href="#predict-the-sensitivity-of-the-likelihood" title="Permalink to this heading">ÔÉÅ</a></h3>
<p>To predict the gradient of the likelihood with respect to time <span class="math notranslate nohighlight">\(t\)</span> and to all parameters,
use <a class="reference internal" href="usage.html#get_sensitivities.sensitivities" title="get_sensitivities.sensitivities"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get_sensitivities.sensitivities()</span></code></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">up_bound</span> <span class="o">=</span> <span class="mi">500</span> <span class="c1"># to choose the upper boundary of the predicted distribution</span>
<span class="n">to_pred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">t</span><span class="p">,</span> <span class="n">theta_1</span><span class="p">,</span> <span class="o">..</span><span class="p">,</span> <span class="n">theta_</span><span class="p">{</span><span class="n">M_theta</span><span class="p">},</span> <span class="n">xi_1_1</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="n">xi_L_</span><span class="p">{</span><span class="n">q_1</span><span class="o">+</span><span class="n">q_2</span><span class="p">}])</span>

<span class="n">y_pred</span> <span class="o">=</span> <span class="n">get_sensitivities</span><span class="o">.</span><span class="n">sensitivities</span><span class="p">(</span><span class="n">to_pred</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">up_bound</span><span class="p">)</span>
</pre></div>
</div>
<p>To get the sensitivity of the likelihood distribution for the <span class="math notranslate nohighlight">\(i^{th}\)</span> parameter:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">y_pred_i</span> <span class="o">=</span> <span class="n">y_pred</span><span class="p">[:,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</section>
</section>
<section id="step-6a-estimate-the-fisher-information">
<h2>Step 6A: Estimate the Fisher Information<a class="headerlink" href="#step-6a-estimate-the-fisher-information" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>The Fisher Information can be computed from the probability mass functions and the sensitivity of the likelihood (see section <a class="reference internal" href="math.html#background-on-the-fisher-information"><span class="std std-ref">Background on the Fisher Information</span></a>).</p>
<p>To estimate the Fisher Information at a single time point <span class="math notranslate nohighlight">\(t\)</span>, use the function <a class="reference internal" href="usage.html#get_fi.fisher_information_t" title="get_fi.fisher_information_t"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get_fi.fisher_information_t()</span></code></a>.
For multiple time points, ie to compute <span class="math notranslate nohighlight">\(\sum\limits_{k=1}^L I_{t_k}^{\theta, \xi}\)</span>, use the function <a class="reference internal" href="usage.html#get_fi.fisher_information" title="get_fi.fisher_information"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get_fi.fisher_information()</span></code></a>.</p>
</section>
<section id="step-6b-estimate-the-expectation-and-its-gradient">
<h2>Step 6B: Estimate the expectation and its gradient<a class="headerlink" href="#step-6b-estimate-the-expectation-and-its-gradient" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>As for the Fisher Information, the expectation and its gradient with respect to the parameters can be computed using the functions
<a class="reference internal" href="usage.html#get_sensitivities.expected_val" title="get_sensitivities.expected_val"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get_sensitivities.expected_val()</span></code></a> and <a class="reference internal" href="usage.html#get_sensitivities.gradient_expected_val" title="get_sensitivities.gradient_expected_val"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get_sensitivities.gradient_expected_val()</span></code></a>.</p>
<p>If a loss function <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> is specified in input, these functions can compute <span class="math notranslate nohighlight">\(\mathcal{L}\big(E_{\theta, \xi}[X_t]\big)\)</span> and
<span class="math notranslate nohighlight">\(\nabla_{t, \theta, \xi} \mathcal{L} \big(E_{\theta, \xi}[X_t]\big) = \frac{dL(x)}{dx} \nabla_{t, \theta, \xi} E_{\theta, \xi}[X_t]\)</span>.</p>
</section>
<section id="step-7-find-the-optimal-control-parameters">
<h2>Step 7: Find the optimal control parameters<a class="headerlink" href="#step-7-find-the-optimal-control-parameters" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>The gradient of the expectation can be used in a Projected Gradient Descent algorithm to find the optimal control parameters that produce a specific species abundance at a given time.</p>
<p>The implementation of this method is in the <a class="reference external" href="https://github.com/gabrielleberrada/DL_based_Control_of_CRNs/blob/main/projected_gradient_descent.py">projected_gradient_descent.py</a> file.
specifically in the class <a class="reference internal" href="usage.html#projected_gradient_descent.ProjectedGradientDescent_MDN" title="projected_gradient_descent.ProjectedGradientDescent_MDN"><code class="xref py py-class docutils literal notranslate"><span class="pre">projected_gradient_descent.ProjectedGradientDescent_MDN</span></code></a> to use with Mixture Density Networks.</p>
<p>For a convenient way to run the algorithm, monitor progress and save results, check out the <a class="reference external" href="https://github.com/gabrielleberrada/DL_based_Control_of_CRNs/blob/main/training_pgd.py">training_pgd.py</a> file.
The function <a class="reference internal" href="usage.html#training_pgd.pgdMDN" title="training_pgd.pgdMDN"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_pgd.pgdMDN()</span></code></a> does the following:</p>
<ul class="simple">
<li><p>Computes the gradient descent.</p></li>
<li><p>Plots various aspects of the algorithm progress, including the optimal control values, loss values as estimated by the model,
control parameter values, sensitivity values, SSA-estimated loss values over iterations and SSA-estimated abundances over time.</p></li>
<li><p>Saves the hyperparameters, parameters and results of the algorithm in a <code class="docutils literal notranslate"><span class="pre">.txt</span></code> file for future reference.</p></li>
</ul>
</section>
<section id="optional-steps">
<h2>Optional steps<a class="headerlink" href="#optional-steps" title="Permalink to this heading">ÔÉÅ</a></h2>
<section id="compare-with-finite-state-projection-results">
<h3>Compare with Finite State Projection results<a class="headerlink" href="#compare-with-finite-state-projection-results" title="Permalink to this heading">ÔÉÅ</a></h3>
<p>You can compare the predictions of the Mixture Density Network to those made using the Finite State Projection (FSP) method.</p>
<p>If the propensity derivatives are not specified, it is assumed that the Chemical Reaction Network follows mass-action kinetics.
However if this is not the case, make sure to specify the derivative functions to compute the sensitivities.</p>
<p>To truncate the state-space, set a value <span class="math notranslate nohighlight">\(C_r \in \mathbb{N}\)</span> such that the <span class="math notranslate nohighlight">\(N_{\max}\)</span>-th element in the enumerated state space corresponds to <span class="math notranslate nohighlight">\((0,...,0,C_r) \in \mathbb{N}^N\)</span>.
Note that <span class="math notranslate nohighlight">\(\Phi_N(0,...,0,C_r) = N_{\max}-1\)</span>. In the case of <span class="math notranslate nohighlight">\(N=2\)</span> species, this means that <span class="math notranslate nohighlight">\(\frac{C_r(C_r+3)}{2}+1 = N_{\max}\)</span>.</p>
<p>The Finite State Projection method is implemented in the <a class="reference external" href="https://github.com/gabrielleberrada/DL_based_Control_of_CRNs/blob/main/fsp.py">fsp.py</a> file.
The method <a class="reference internal" href="usage.html#fsp.SensitivitiesDerivation.solve_multiple_odes" title="fsp.SensitivitiesDerivation.solve_multiple_odes"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fsp.SensitivitiesDerivation.solve_multiple_odes()</span></code></a> computes the probability mass functions and the sensitivity of the likelihood.</p>
<p>Keep in mind that the Finite State Projection method estimates global probability mass functions, while Mixture Density Networks predict marginal probability mass functions.
To make a fair comparison, compute the marginal probability mass functions using either the method <a class="reference internal" href="usage.html#fsp.SensitivitiesDerivation.marginal" title="fsp.SensitivitiesDerivation.marginal"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fsp.SensitivitiesDerivation.marginal()</span></code></a> for a single
mass function or the method <a class="reference internal" href="usage.html#fsp.SensitivitiesDerivation.marginals" title="fsp.SensitivitiesDerivation.marginals"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fsp.SensitivitiesDerivation.marginals()</span></code></a> for multiple mass functions.</p>
<p>To compute the expectation and its gradient, use the methods <a class="reference internal" href="usage.html#fsp.SensitivitiesDerivation.expected_val" title="fsp.SensitivitiesDerivation.expected_val"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fsp.SensitivitiesDerivation.expected_val()</span></code></a> and <a class="reference internal" href="usage.html#fsp.SensitivitiesDerivation.gradient_expected_val" title="fsp.SensitivitiesDerivation.gradient_expected_val"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fsp.SensitivitiesDerivation.gradient_expected_val()</span></code></a>, respectively.</p>
<p>To run the Projected Gradient Descent using the Finite State Projection Method, use the class <a class="reference internal" href="usage.html#projected_gradient_descent.ProjectedGradientDescent_FSP" title="projected_gradient_descent.ProjectedGradientDescent_FSP"><code class="xref py py-class docutils literal notranslate"><span class="pre">projected_gradient_descent.ProjectedGradientDescent_FSP</span></code></a>. Call the function <a class="reference internal" href="usage.html#training_pgd.pgdFSP" title="training_pgd.pgdFSP"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_pgd.pgdFSP()</span></code></a>
to perform the gradient descent and save the results.</p>
</section>
<section id="use-regularisation-methods">
<h3>Use regularisation methods<a class="headerlink" href="#use-regularisation-methods" title="Permalink to this heading">ÔÉÅ</a></h3>
<p>In all computed examples, we have not come across overfitting. If it arises, the regularisation methods provided in the code can be applied to address it:</p>
<ul class="simple">
<li><p>Add a <span class="math notranslate nohighlight">\(\ell_2\)</span>-regularisation term.</p></li>
<li><p>Use an early stopping method. You need to specify a tolerance threshold <span class="math notranslate nohighlight">\(\delta\)</span> as well as a patience level <span class="math notranslate nohighlight">\(n_p\)</span>.
See section <a class="reference internal" href="advice.html#some-advice-on-the-implementation-of-the-approach"><span class="std std-ref">How to deal with overfitting?</span></a> for more details on this method.</p></li>
</ul>
</section>
<section id="tune-hyperparameters">
<h3>Tune hyperparameters<a class="headerlink" href="#tune-hyperparameters" title="Permalink to this heading">ÔÉÅ</a></h3>
<p>To optimise the results, it is important to find the best hyperparameters.</p>
<p>Here are some examples of hyperparameters to tune:</p>
<ul class="simple">
<li><p>Batchsize</p></li>
<li><p>Learning rate</p></li>
<li><p>Mixture type</p></li>
<li><p>Number of components</p></li>
<li><p>Number of hidden layer neurons</p></li>
<li><p>Number of samples in the training dataset</p></li>
<li><p>Number of training rounds</p></li>
<li><p>Patience and delta in case of early stopping</p></li>
</ul>
<p>To tune them, train models for each parameters combination. Use the validation dataset to estimate the optimal set of parameters.
To speed up the process, consider using multiprocessing.</p>
<p>In our demos, we tuned the learning rate, number of training rounds, number of hidden layer neurons and batchsize.
We kept the number of mixture components separate.</p>
<p>The implemented code is available in the <a class="reference external" href="https://github.com/gabrielleberrada/DL_based_Control_of_CRNs/blob/main/tuning.py">tuning.py</a> file. It is easily adaptable to any Chemical Reaction Network. This file
uses the function <a class="reference internal" href="usage.html#hyperparameters_tuning.test_multiple_combs" title="hyperparameters_tuning.test_multiple_combs"><code class="xref py py-meth docutils literal notranslate"><span class="pre">hyperparameters_tuning.test_multiple_combs()</span></code></a>  which trains one or several models and
saves the results of all parameter combinations in a CSV file. It calls the function <a class="reference internal" href="usage.html#hyperparameters_test.test_comb" title="hyperparameters_test.test_comb"><code class="xref py py-meth docutils literal notranslate"><span class="pre">hyperparameters_test.test_comb()</span></code></a> to test each combination.</p>
<p>A similar method can be used to tune the Projected Gradient Descent algorithm hyperparameters.</p>
</section>
<section id="plot-probability-distributions-and-sensitivities-distributions">
<h3>Plot probability distributions and sensitivities distributions<a class="headerlink" href="#plot-probability-distributions-and-sensitivities-distributions" title="Permalink to this heading">ÔÉÅ</a></h3>
<p>Evaluating the accuracy of a model based only on loss values can be challenging. A quicker and more intuitive way to assess model performance is by visualizing the predicted distributions and comparing them to other distributions.
This can include a known exact distribution if the Chemical Reaction Network has a known sampled mass function, simulated distributions from Stochastic Algorithms, or even distirbutions estimated by the Finite State Projection method.</p>
<p>To plot the results, you can use the functions in the <a class="reference external" href="https://github.com/gabrielleberrada/DL_based_Control_of_CRNs/blob/main/plot.py">plot.py</a> file. For a single plot, call the function
<a class="reference internal" href="usage.html#plot.plot_model" title="plot.plot_model"><code class="xref py py-meth docutils literal notranslate"><span class="pre">plot.plot_model()</span></code></a>. To compare multiple distributions, call the function <a class="reference internal" href="usage.html#plot.multiple_plots" title="plot.multiple_plots"><code class="xref py py-meth docutils literal notranslate"><span class="pre">plot.multiple_plots()</span></code></a>.</p>
<p>These functions allow you to plot both probability distributions and sensitivity distributions, giving a comprehensive view of the model performance.</p>
<p>Examples can be found in the notebooks on the <a class="reference external" href="https://github.com/gabrielleberrada/DL_based_Control_of_CRNs">GitHub repository</a>.</p>
</section>
<section id="plot-the-fisher-information-values-in-a-table-or-in-barplots">
<h3>Plot the Fisher Information values in a table or in barplots<a class="headerlink" href="#plot-the-fisher-information-values-in-a-table-or-in-barplots" title="Permalink to this heading">ÔÉÅ</a></h3>
<p>To compare and see the Fisher Information results from different sources, a comprehensive table can be created by calling the function <a class="reference internal" href="usage.html#plot.fi_table" title="plot.fi_table"><code class="xref py py-meth docutils literal notranslate"><span class="pre">plot.fi_table()</span></code></a>.
It brings together the predictions from the Mixture Density Network, the calculations from the Finite State Projection, and, when available, the exact results.</p>
<p>For a more visual representation, you can use bar plots, generated by calling the function <code class="xref py py-meth docutils literal notranslate"><span class="pre">fi_barplots()</span></code>.</p>
<p>Examples can be found in all the notebooks on the <a class="reference external" href="https://github.com/gabrielleberrada/DL_based_Control_of_CRNs">GitHub repository</a>.</p>
</section>
<section id="plot-the-expectation-and-its-gradient-in-a-table-or-in-barplots">
<h3>Plot the expectation and its gradient in a table or in barplots<a class="headerlink" href="#plot-the-expectation-and-its-gradient-in-a-table-or-in-barplots" title="Permalink to this heading">ÔÉÅ</a></h3>
<p>Just as for the Fisher Information values, the results of the expectation and its gradient values, evaluated using different methods, can be compared in tables or barplots.
To do so, call the functions <a class="reference internal" href="usage.html#plot.expect_val_table" title="plot.expect_val_table"><code class="xref py py-meth docutils literal notranslate"><span class="pre">plot.expect_val_table()</span></code></a> and <a class="reference internal" href="usage.html#plot.expect_val_barplots" title="plot.expect_val_barplots"><code class="xref py py-meth docutils literal notranslate"><span class="pre">plot.expect_val_barplots()</span></code></a>.</p>
<p>Examples can be found in all the notebooks on the <a class="reference external" href="https://github.com/gabrielleberrada/DL_based_Control_of_CRNs">GitHub repository</a>.</p>
</section>
<section id="store-and-load-mixture-density-networks-weights">
<h3>Store and load Mixture Density Networks weights<a class="headerlink" href="#store-and-load-mixture-density-networks-weights" title="Permalink to this heading">ÔÉÅ</a></h3>
<p>A trained Mixture Density Network model can be saved and loaded at any time.</p>
<p>To do so, use the <a class="reference external" href="https://github.com/gabrielleberrada/DL_based_Control_of_CRNs/blob/main/save_load_MDN.py">save_load_MDN.py</a> file.
To save all needed information to define the Mixture Density Network in a <cite>.pt</cite> file, call the function <code class="xref py py-meth docutils literal notranslate"><span class="pre">save_MDN_model()</span></code>.
To load a Mixture Density Network from a <cite>.pt</cite> file, call the function <a class="reference internal" href="usage.html#save_load_MDN.load_MDN_model" title="save_load_MDN.load_MDN_model"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_load_MDN.load_MDN_model()</span></code></a>.</p>
</section>
<section id="estimate-the-loss-of-the-projected-gradient-descent-based-on-stochastic-simulations">
<h3>Estimate the loss of the Projected Gradient Descent based on Stochastic Simulations<a class="headerlink" href="#estimate-the-loss-of-the-projected-gradient-descent-based-on-stochastic-simulations" title="Permalink to this heading">ÔÉÅ</a></h3>
<p>Given a parameter configuration <span class="math notranslate nohighlight">\(\theta\)</span> and <span class="math notranslate nohighlight">\(\xi\)</span>, the true loss value (as estimated by Stochastic Simulations)
can be computed as following:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sim</span> <span class="o">=</span> <span class="n">generate_data</span><span class="o">.</span><span class="n">CRN_Simulations</span><span class="p">(</span><span class="n">crn</span><span class="o">=</span><span class="n">crn</span><span class="p">,</span>
                                    <span class="n">time_windows</span><span class="o">=</span><span class="n">time_windows</span><span class="p">,</span>
                                    <span class="n">n_trajectories</span><span class="o">=</span><span class="mi">10</span><span class="o">**</span><span class="mi">4</span><span class="p">,</span>
                                    <span class="n">ind_species</span><span class="o">=</span><span class="n">ind_species</span><span class="p">,</span>
                                    <span class="n">complete_trajectory</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                    <span class="n">sampling_times</span><span class="o">=</span><span class="n">time_windows</span><span class="p">)</span>
<span class="n">parameters</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">fixed_parameters</span><span class="p">,</span> <span class="n">control_parameters</span><span class="p">))</span>
<span class="n">samples</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">sim</span><span class="o">.</span><span class="n">run_simulations</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span>
<span class="n">expect</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">res</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_time_windows</span><span class="p">):</span>
    <span class="n">res</span> <span class="o">+=</span> <span class="n">weights</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">loss_function</span><span class="p">[</span><span class="n">j</span><span class="p">](</span><span class="n">expect</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
</pre></div>
</div>
<p>The method to compute the real loss evolution based on Stochastic Simulations also is implemented in
<a class="reference internal" href="usage.html#projected_gradient_descent.ProjectedGradientDescent_CRN.plot_performance_index" title="projected_gradient_descent.ProjectedGradientDescent_CRN.plot_performance_index"><code class="xref py py-meth docutils literal notranslate"><span class="pre">projected_gradient_descent.ProjectedGradientDescent_CRN.plot_performance_index()</span></code></a>.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="index.html" class="btn btn-neutral float-left" title="Efficient Fisher Information Computation and Policy Srearch in Sampled Stochastic Chemical Reaction Networks through Deep Learning" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="demos.html" class="btn btn-neutral float-right" title="Examples of Chemical Reaction Networks" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Control Theory and Systems Biology Laboratory, D-BSSE, ETH Zurich.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>