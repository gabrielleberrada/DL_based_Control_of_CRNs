<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>API Documentation &mdash; Computing the Fisher Information of CRNs using Deep Learning 0.1 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/sphinx_highlight.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Background on the Architecture of the Mixture Density Network" href="neuralnetwork.html" />
    <link rel="prev" title="Some advice on the implementation of the approach" href="advice.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> Computing the Fisher Information of CRNs using Deep Learning
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="workflow.html">Workflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="demos.html">Examples of Chemical Reaction Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="advice.html">Some advice on the implementation of the approach</a></li>
</ul>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">API Documentation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#requirements">Requirements</a></li>
<li class="toctree-l2"><a class="reference internal" href="#simulating-chemical-reaction-networks-using-stochastic-simulation-algorithms">Simulating Chemical Reaction Networks using Stochastic Simulation Algorithms</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#simulation.CRN"><code class="docutils literal notranslate"><span class="pre">CRN</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#simulation.CRN.reset"><code class="docutils literal notranslate"><span class="pre">CRN.reset()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#simulation.CRN.simulation"><code class="docutils literal notranslate"><span class="pre">CRN.simulation()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#simulation.CRN.step"><code class="docutils literal notranslate"><span class="pre">CRN.step()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#simulation.StochasticSimulation"><code class="docutils literal notranslate"><span class="pre">StochasticSimulation</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#simulation.StochasticSimulation.SSA"><code class="docutils literal notranslate"><span class="pre">StochasticSimulation.SSA()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#simulation.StochasticSimulation.mNRM"><code class="docutils literal notranslate"><span class="pre">StochasticSimulation.mNRM()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#multiple-simulations">Multiple simulations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#generate_data.CRN_Dataset"><code class="docutils literal notranslate"><span class="pre">CRN_Dataset</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#generate_data.CRN_Simulations"><code class="docutils literal notranslate"><span class="pre">CRN_Simulations</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#saving-data">Saving data</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#convert_csv.array_to_csv"><code class="docutils literal notranslate"><span class="pre">array_to_csv()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#convert_csv.csv_to_array"><code class="docutils literal notranslate"><span class="pre">csv_to_array()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#convert_csv.csv_to_tensor"><code class="docutils literal notranslate"><span class="pre">csv_to_tensor()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#generate_csv.generate_csv_datasets"><code class="docutils literal notranslate"><span class="pre">generate_csv_datasets()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#generate_csv.generate_csv_simulations"><code class="docutils literal notranslate"><span class="pre">generate_csv_simulations()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#building-and-training-a-mixture-density-network">Building and training a Mixture Density Network</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#neuralnetwork.NeuralNetwork"><code class="docutils literal notranslate"><span class="pre">NeuralNetwork</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#neuralnetwork.NeuralNetwork.forward"><code class="docutils literal notranslate"><span class="pre">NeuralNetwork.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#neuralnetwork.distr_pdf"><code class="docutils literal notranslate"><span class="pre">distr_pdf()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#neuralnetwork.mix_pdf"><code class="docutils literal notranslate"><span class="pre">mix_pdf()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#neuralnetwork.loss_kldivergence"><code class="docutils literal notranslate"><span class="pre">loss_kldivergence()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#neuralnetwork.loss_hellinger"><code class="docutils literal notranslate"><span class="pre">loss_hellinger()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#neuralnetwork.mean_loss"><code class="docutils literal notranslate"><span class="pre">mean_loss()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#neuralnetwork.NNTrainer"><code class="docutils literal notranslate"><span class="pre">NNTrainer</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#neuralnetwork.NNTrainer.early_stopping"><code class="docutils literal notranslate"><span class="pre">NNTrainer.early_stopping()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#neuralnetwork.NNTrainer.update_losses"><code class="docutils literal notranslate"><span class="pre">NNTrainer.update_losses()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#neuralnetwork.train_round"><code class="docutils literal notranslate"><span class="pre">train_round()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#neuralnetwork.train_NN"><code class="docutils literal notranslate"><span class="pre">train_NN()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#computing-probability-mass-functions-and-the-sensitivity-of-the-likelihood">Computing probability mass functions and the sensitivity of the likelihood</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#get_sensitivities.probabilities"><code class="docutils literal notranslate"><span class="pre">probabilities()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#get_sensitivities.sensitivities"><code class="docutils literal notranslate"><span class="pre">sensitivities()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#computing-the-fisher-information">Computing the Fisher Information</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#get_fi.fisher_information_t"><code class="docutils literal notranslate"><span class="pre">fisher_information_t()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#get_fi.fisher_information"><code class="docutils literal notranslate"><span class="pre">fisher_information()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#computing-the-expectation-and-its-gradient">Computing the expectation and its gradient</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#get_sensitivities.expected_val"><code class="docutils literal notranslate"><span class="pre">expected_val()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#get_sensitivities.gradient_expected_val"><code class="docutils literal notranslate"><span class="pre">gradient_expected_val()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#performing-a-projected-gradient-descent">Performing a Projected Gradient Descent</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#projected_gradient_descent.ProjectedGradientDescent"><code class="docutils literal notranslate"><span class="pre">ProjectedGradientDescent</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#projected_gradient_descent.ProjectedGradientDescent.projected_gradient_descent"><code class="docutils literal notranslate"><span class="pre">ProjectedGradientDescent.projected_gradient_descent()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#projected_gradient_descent.ProjectedGradientDescent_CRN"><code class="docutils literal notranslate"><span class="pre">ProjectedGradientDescent_CRN</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#projected_gradient_descent.ProjectedGradientDescent_CRN.optimisation"><code class="docutils literal notranslate"><span class="pre">ProjectedGradientDescent_CRN.optimisation()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#projected_gradient_descent.ProjectedGradientDescent_CRN.plot_abundances"><code class="docutils literal notranslate"><span class="pre">ProjectedGradientDescent_CRN.plot_abundances()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#projected_gradient_descent.ProjectedGradientDescent_CRN.plot_control_params_trajectory"><code class="docutils literal notranslate"><span class="pre">ProjectedGradientDescent_CRN.plot_control_params_trajectory()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#projected_gradient_descent.ProjectedGradientDescent_CRN.plot_control_values"><code class="docutils literal notranslate"><span class="pre">ProjectedGradientDescent_CRN.plot_control_values()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#projected_gradient_descent.ProjectedGradientDescent_CRN.plot_gradients_trajectory"><code class="docutils literal notranslate"><span class="pre">ProjectedGradientDescent_CRN.plot_gradients_trajectory()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#projected_gradient_descent.ProjectedGradientDescent_CRN.plot_losses_trajectory"><code class="docutils literal notranslate"><span class="pre">ProjectedGradientDescent_CRN.plot_losses_trajectory()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#projected_gradient_descent.ProjectedGradientDescent_CRN.plot_performance_index"><code class="docutils literal notranslate"><span class="pre">ProjectedGradientDescent_CRN.plot_performance_index()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#projected_gradient_descent.ProjectedGradientDescent_MDN"><code class="docutils literal notranslate"><span class="pre">ProjectedGradientDescent_MDN</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#projected_gradient_descent.ProjectedGradientDescent_MDN.create_gradient"><code class="docutils literal notranslate"><span class="pre">ProjectedGradientDescent_MDN.create_gradient()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#projected_gradient_descent.ProjectedGradientDescent_MDN.create_loss"><code class="docutils literal notranslate"><span class="pre">ProjectedGradientDescent_MDN.create_loss()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#projected_gradient_descent.control_method"><code class="docutils literal notranslate"><span class="pre">control_method()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#training_pgd.pgdMDN"><code class="docutils literal notranslate"><span class="pre">pgdMDN()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#tuning-hyperparameters">Tuning hyperparameters</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#hyperparameters_test.test_comb"><code class="docutils literal notranslate"><span class="pre">test_comb()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#hyperparameters_tuning.test_multiple_combs"><code class="docutils literal notranslate"><span class="pre">test_multiple_combs()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#comparing-with-the-finite-state-projection-method">Comparing with the Finite State Projection method</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#fsp.StateSpaceEnumeration"><code class="docutils literal notranslate"><span class="pre">StateSpaceEnumeration</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#fsp.StateSpaceEnumeration.create_bijection"><code class="docutils literal notranslate"><span class="pre">StateSpaceEnumeration.create_bijection()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#fsp.StateSpaceEnumeration.phi"><code class="docutils literal notranslate"><span class="pre">StateSpaceEnumeration.phi()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#fsp.StateSpaceEnumeration.phi_inverse"><code class="docutils literal notranslate"><span class="pre">StateSpaceEnumeration.phi_inverse()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#fsp.SensitivitiesDerivation"><code class="docutils literal notranslate"><span class="pre">SensitivitiesDerivation</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#fsp.SensitivitiesDerivation.constant_matrix"><code class="docutils literal notranslate"><span class="pre">SensitivitiesDerivation.constant_matrix()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#fsp.SensitivitiesDerivation.create_gdrv"><code class="docutils literal notranslate"><span class="pre">SensitivitiesDerivation.create_gdrv()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#fsp.SensitivitiesDerivation.create_gdrv_B"><code class="docutils literal notranslate"><span class="pre">SensitivitiesDerivation.create_gdrv_B()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#fsp.SensitivitiesDerivation.create_generator"><code class="docutils literal notranslate"><span class="pre">SensitivitiesDerivation.create_generator()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#fsp.SensitivitiesDerivation.create_generator_derivative"><code class="docutils literal notranslate"><span class="pre">SensitivitiesDerivation.create_generator_derivative()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#fsp.SensitivitiesDerivation.expected_val"><code class="docutils literal notranslate"><span class="pre">SensitivitiesDerivation.expected_val()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#fsp.SensitivitiesDerivation.gradient_expected_val"><code class="docutils literal notranslate"><span class="pre">SensitivitiesDerivation.gradient_expected_val()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#fsp.SensitivitiesDerivation.marginal"><code class="docutils literal notranslate"><span class="pre">SensitivitiesDerivation.marginal()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#fsp.SensitivitiesDerivation.marginals"><code class="docutils literal notranslate"><span class="pre">SensitivitiesDerivation.marginals()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#fsp.SensitivitiesDerivation.reset"><code class="docutils literal notranslate"><span class="pre">SensitivitiesDerivation.reset()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#fsp.SensitivitiesDerivation.solve_multiple_odes"><code class="docutils literal notranslate"><span class="pre">SensitivitiesDerivation.solve_multiple_odes()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#fsp.SensitivitiesDerivation.solve_ode"><code class="docutils literal notranslate"><span class="pre">SensitivitiesDerivation.solve_ode()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#projected_gradient_descent.ProjectedGradientDescent_FSP"><code class="docutils literal notranslate"><span class="pre">ProjectedGradientDescent_FSP</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#projected_gradient_descent.ProjectedGradientDescent_FSP.create_gradient"><code class="docutils literal notranslate"><span class="pre">ProjectedGradientDescent_FSP.create_gradient()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#projected_gradient_descent.ProjectedGradientDescent_FSP.create_loss"><code class="docutils literal notranslate"><span class="pre">ProjectedGradientDescent_FSP.create_loss()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#training_pgd.pgdFSP"><code class="docutils literal notranslate"><span class="pre">pgdFSP()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#plotting-distributions-fisher-information-and-expectations">Plotting distributions, Fisher Information and expectations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#plot.plot_model"><code class="docutils literal notranslate"><span class="pre">plot_model()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#plot.multiple_plots"><code class="docutils literal notranslate"><span class="pre">multiple_plots()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#plotting-fisher-information-results">Plotting Fisher Information results</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#plot.fi_table"><code class="docutils literal notranslate"><span class="pre">fi_table()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#plot.fi_barplots"><code class="docutils literal notranslate"><span class="pre">fi_barplots()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#plotting-expectation-results">Plotting expectation results</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#plot.expect_val_table"><code class="docutils literal notranslate"><span class="pre">expect_val_table()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#plot.expect_val_barplots"><code class="docutils literal notranslate"><span class="pre">expect_val_barplots()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#saving-and-loading-a-mixture-density-network">Saving and loading a Mixture Density Network</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#save_load_MDN.save_MDN_model"><code class="docutils literal notranslate"><span class="pre">save_MDN_model()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#save_load_MDN.load_MDN_model"><code class="docutils literal notranslate"><span class="pre">load_MDN_model()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="neuralnetwork.html">Background on the Architecture of the Mixture Density Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="control.html">Background on the Stochastic control of Chemical Reaction Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="math.html">Background on the Fisher Information</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="sources.html">Sources</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Computing the Fisher Information of CRNs using Deep Learning</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>API Documentation</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/usage.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="api-documentation">
<h1>API Documentation<a class="headerlink" href="#api-documentation" title="Permalink to this heading"></a></h1>
<section id="requirements">
<h2>Requirements<a class="headerlink" href="#requirements" title="Permalink to this heading"></a></h2>
<p>This framework requires the installation of:</p>
<ul class="simple">
<li><p>Concurrent.futures</p></li>
<li><p>Matplotlib</p></li>
<li><p>NumPy</p></li>
<li><p>PyTorch</p></li>
<li><p>SciPy</p></li>
</ul>
<p>See the file <a class="reference external" href="https://github.com/gabrielleberrada/DL_based_Control_of_CRNs/blob/main/requirements.txt">requirements.txt</a> for more details on the package requirements.</p>
<p>This section outlines all the classes and functions used in the workflow.</p>
<p>Some useful notations are gathered in the following table:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 76%" />
<col style="width: 24%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p>Denomination</p></td>
<td><p>Notation</p></td>
</tr>
<tr class="row-even"><td><p>Fisher Information at time <span class="math notranslate nohighlight">\(t\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(I_t^{\theta,\xi}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Number of control parameters of the propensity functions <span class="math notranslate nohighlight">\(\xi\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(M_{\xi}\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Number of elements in the case of a finite state-space</p></td>
<td><p><span class="math notranslate nohighlight">\(N_{\max}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Number of fixed parameters of the propensity functions <span class="math notranslate nohighlight">\(\theta\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(M_{\theta}\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Number of reactions</p></td>
<td><p><span class="math notranslate nohighlight">\(M\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Number of species</p></td>
<td><p><span class="math notranslate nohighlight">\(N\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Number of time windows</p></td>
<td><p><span class="math notranslate nohighlight">\(L\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Probability mass function evaluated at the <span class="math notranslate nohighlight">\(\ell\)</span>-th element of the enumerated state-space</p></td>
<td><p><span class="math notranslate nohighlight">\(p_\ell(t, \theta, \xi)\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Sensitivity of the likelihood matrix</p></td>
<td><p><span class="math notranslate nohighlight">\(S_t^{\theta,\xi}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Total number of parameters of the propensity functions <span class="math notranslate nohighlight">\(M_{\theta} + M_{\xi}\times L\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(M_{\text{tot}}\)</span></p></td>
</tr>
</tbody>
</table>
</section>
<section id="simulating-chemical-reaction-networks-using-stochastic-simulation-algorithms">
<h2>Simulating Chemical Reaction Networks using Stochastic Simulation Algorithms<a class="headerlink" href="#simulating-chemical-reaction-networks-using-stochastic-simulation-algorithms" title="Permalink to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="simulation.CRN">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">simulation.</span></span><span class="sig-name descname"><span class="pre">CRN</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stoichiometry_mat</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">propensities</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_fixed_params</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_control_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">propensities_drv</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exact_distr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(False,</span> <span class="pre">None,</span> <span class="pre">None)</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/simulation.html#CRN"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#simulation.CRN" title="Permalink to this definition"></a></dt>
<dd><p>Class to specify the CRN to work on.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>stoichiometry_mat</strong> (np.ndarray): Stoichiometry matrix. Has shape <span class="math notranslate nohighlight">\((N, M)\)</span>.</p></li>
<li><p><strong>propensities</strong> (np.ndarray): Non-parameterised propensity functions.</p></li>
<li><p><strong>init_state</strong> (np.ndarray): Initial state of the system.</p></li>
<li><p><strong>n_fixed_params</strong> (int): Number of fixed parameters required to define the propensity functions <span class="math notranslate nohighlight">\(M_{\theta}\)</span>.</p></li>
<li><p><strong>n_control_params</strong> (int): Number of control parameters required to define the propensity functions <span class="math notranslate nohighlight">\(M_{\xi}\)</span>. 
Their values vary from one time window to another.</p></li>
<li><p><strong>propensities_drv</strong> (np.ndarray, optional): Gradient functions of the propensities with respect to the parameters.
Has shape <span class="math notranslate nohighlight">\((M, M_{\theta}+M_{\xi})\)</span>. If None, the CRN is assumed to follow mass-action kinetics.
Defaults to None.</p></li>
<li><p><strong>exact</strong> (Tuple[bool, Tuple[Callable], Tuple[Callable]], optional): If the first element is True, 
the exact distribution of the CRN is known. The second element then is the exact probability mass function. The third
element is the exact sensitivities of the mass function. Defaults to (False, None, None).</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="simulation.CRN.reset">
<span class="sig-name descname"><span class="pre">reset</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/simulation.html#CRN.reset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#simulation.CRN.reset" title="Permalink to this definition"></a></dt>
<dd><p>Resets the CRN to the initial setting: sets the time to <span class="math notranslate nohighlight">\(t=0\)</span>, the current state to the initial state and
empties the sampling times and samples arrays.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="simulation.CRN.simulation">
<span class="sig-name descname"><span class="pre">simulation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sampling_times</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_windows</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parameters</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'SSA'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">complete_trajectory</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/simulation.html#CRN.simulation"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#simulation.CRN.simulation" title="Permalink to this definition"></a></dt>
<dd><p>Computes a simulation between two time points.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>sampling_times</strong> (np.ndarray): Sampling times.</p></li>
<li><p><strong>time_windows</strong> (np.ndarray): Time windows during which all parameters are constant. Its form is <span class="math notranslate nohighlight">\([t_1, ..., t_L]\)</span>,
such that the time windows are <span class="math notranslate nohighlight">\([0, t_1], [t_1, t_2], ..., [t_{L-1}, t_L]\)</span>. <span class="math notranslate nohighlight">\(t_L\)</span> must match
with the final time <span class="math notranslate nohighlight">\(t_f\)</span>. If there is only one time window, it should be defined as <span class="math notranslate nohighlight">\([t_f]\)</span>.</p></li>
<li><p><strong>parameters</strong> (np.ndarray): Parameters of the simulation, including fixed parameters for the whole simulation and control
parameters for each time window. Its form is <span class="math notranslate nohighlight">\([\theta_1, ..., \theta_{M_{\theta}}, \xi_1^1, ..., \xi^{M_{\xi}}_1, \xi_2^1, ..., \xi_L^{M_{\xi}}]\)</span>.
Has shape <span class="math notranslate nohighlight">\((M_{\tot},)\)</span>.</p></li>
<li><p><strong>method</strong> (str, optional): Stochastic Simulation to compute. Defaults to <cite>SSA</cite>.</p></li>
<li><p><strong>complete_trajectory</strong> (bool, optional): If True, saves the complete trajectory of the simulation, ie the time of each jump and the
corresponding abundance. Defaults to False.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="simulation.CRN.step">
<span class="sig-name descname"><span class="pre">step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">init_state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">params</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sampling_times</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">t0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tf</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">complete_trajectory</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/simulation.html#CRN.step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#simulation.CRN.step" title="Permalink to this definition"></a></dt>
<dd><p>Computes a simulation for a time window during which all parameters are constant.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>init_state</strong> (np.ndarray): Initial state of the CRN when the simulation starts.</p></li>
<li><p><strong>params</strong> (np.ndarray): Parameters associated to the propensity functions.</p></li>
<li><p><strong>sampling_times</strong> (np.ndarray): Sampling times.</p></li>
<li><p><span class="math notranslate nohighlight">\(t_0\)</span> (float): Time at which the simulation starts.</p></li>
<li><p><span class="math notranslate nohighlight">\(t_f\)</span> (float): Time to end the simulation.</p></li>
<li><p><strong>method</strong> (str): Stochastic Simulation to compute.</p></li>
<li><p><strong>complete_trajectory</strong> (bool): If True, saves the complete trajectory of the simulation, ie the time of each jump and the
corresponding abundance.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="simulation.StochasticSimulation">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">simulation.</span></span><span class="sig-name descname"><span class="pre">StochasticSimulation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">t0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tf</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sampling_times</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">propensities</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_species</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_reactions</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stoich_mat</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/simulation.html#StochasticSimulation"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#simulation.StochasticSimulation" title="Permalink to this definition"></a></dt>
<dd><p>Class to run a simulation between two time points of the same time window using the Stochastic Simulation Algorithm <span id="id1">[<a class="reference internal" href="sources.html#id7" title="Daniel T Gillespie. A general method for numerically simulating the stochastic time evolution of coupled chemical reactions. Journal of Computational Physics., 22:403–434, 1976.">Gil76</a>]</span>.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><span class="math notranslate nohighlight">\(x_0\)</span> (np.ndarray): Initial state.</p></li>
<li><p><span class="math notranslate nohighlight">\(t_0\)</span> (float): Initial time of the simulation.</p></li>
<li><p><span class="math notranslate nohighlight">\(t_f\)</span> (float): Final time of the simulation.</p></li>
<li><p><strong>sampling_times</strong> (np.ndarray): Sampling times.</p></li>
<li><p><strong>propensities</strong> (np.ndarray): Parameterised propensity functions.</p></li>
<li><p><strong>n_species</strong> (int): Number of species involved <span class="math notranslate nohighlight">\(N\)</span>.</p></li>
<li><p><strong>n_reactions</strong> (int): Number of reactions of the CRN <span class="math notranslate nohighlight">\(M\)</span>.</p></li>
<li><p><strong>stoich_mat</strong> (np.ndarray): Stoichiometry matrix.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="simulation.StochasticSimulation.SSA">
<span class="sig-name descname"><span class="pre">SSA</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">complete_trajectory</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/simulation.html#StochasticSimulation.SSA"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#simulation.StochasticSimulation.SSA" title="Permalink to this definition"></a></dt>
<dd><p>Computes the SSA.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>complete_trajectory</strong> (bool): If True, returns the complete jump process, ie the time
of each jump and the corresponding abundance. The jump times can be found in the attribute <strong>sampling_times</strong>.
Defaults to False.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p><strong>samples</strong>: Abundance samples at the sampling times.</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="simulation.StochasticSimulation.mNRM">
<span class="sig-name descname"><span class="pre">mNRM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">complete_trajectory</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/simulation.html#StochasticSimulation.mNRM"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#simulation.StochasticSimulation.mNRM" title="Permalink to this definition"></a></dt>
<dd><p>Computes the mNRM.</p>
<p>To be implemented.</p>
</dd></dl>

</dd></dl>

<section id="multiple-simulations">
<h3>Multiple simulations<a class="headerlink" href="#multiple-simulations" title="Permalink to this heading"></a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="generate_data.CRN_Dataset">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">generate_data.</span></span><span class="sig-name descname"><span class="pre">CRN_Dataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">crn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sampling_times</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_windows</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_trajectories</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ind_species</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'SSA'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/generate_data.html#CRN_Dataset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#generate_data.CRN_Dataset" title="Permalink to this definition"></a></dt>
<dd><p>Class to build a dataset of probability distributions for a specified CRN.</p>
<dl class="simple">
<dt>Args: </dt><dd><ul class="simple">
<li><p><strong>crn</strong> (simulation.CRN): CRN to work on.</p></li>
<li><p><strong>sampling_times</strong> (np.ndarray): Sampling times.</p></li>
<li><p><strong>time_windows</strong> (np.ndarray): Time windows during which all parameters are constant. Its form is <span class="math notranslate nohighlight">\([t_1, ..., t_L]\)</span>,
such that the time windows are <span class="math notranslate nohighlight">\([0, t_1], [t_1, t_2], ..., [t_{L-1}, t_L]\)</span>. <span class="math notranslate nohighlight">\(t_L\)</span> must match
with the final time <span class="math notranslate nohighlight">\(t_f\)</span>. If there is only one time window, it should be defined as <span class="math notranslate nohighlight">\([t_f]\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(n_{\text{trajectories}}\)</span> (int, optional): Number of trajectories to compute. 
Can also be defined when calling the <code class="docutils literal notranslate"><span class="pre">generate_data</span></code> function. Defaults to <span class="math notranslate nohighlight">\(10^4\)</span>.</p></li>
<li><p><strong>ind_species</strong> (int, optional): Index of the species of interest. 
The distribution generated will be the one of that species. Defaults to <span class="math notranslate nohighlight">\(0\)</span>.</p></li>
<li><p><strong>method</strong> (str, optional): Stochastic Simulation to compute. Defaults to <cite>SSA</cite>.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="generate_data.CRN_Dataset.generate_data">
<span class="sig-name descname"><span class="pre">generate_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data_length</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sobol_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sobol_end</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_trajectories</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10000</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/generate_data.html#CRN_Dataset.generate_data"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#generate_data.CRN_Dataset.generate_data" title="Permalink to this definition"></a></dt>
<dd><p>Generates a dataset which can be used for training, validation or testing.
Uses multiprocessing to run multiple simulations in parallel.
Parameters are generated from the Sobol Sequence (Low Discrepancy Sequence).</p>
<dl>
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>data_length</strong> (int): Length of the expected output data.</p></li>
<li><p><strong>sobol_start</strong> (np.ndarray): Lower boundaries of the parameters samples. Has shape <span class="math notranslate nohighlight">\((M_{\text{tot}},)\)</span>.
If None, an array of zeros. Defaults to None.</p></li>
<li><p><strong>sobol_end</strong> (np.ndarray): Upper boundaries of the parameters samples. Has shape <span class="math notranslate nohighlight">\((M_{\text{tot}},)\)</span>.
If None, an array of ones. Defaults to None.</p></li>
<li><p><span class="math notranslate nohighlight">\(n_{\text{trajectories}}\)</span> (int, optional): Number of trajectories to compute to estimate the distribution. 
Defaults to <span class="math notranslate nohighlight">\(10^4\)</span>.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul>
<li><p><strong>(X, y)</strong>:</p>
<blockquote>
<div><ul class="simple">
<li><p>Each entry of <strong>X</strong> is an input to the neural network of the form 
<span class="math notranslate nohighlight">\([t, \theta_1,..., \theta_{M_{\theta}}, \xi_1^1, ..., \xi_1^{M_{\xi}}, \xi_2^1, ..., \xi^{M_{\xi}}_L]\)</span>.</p></li>
<li><p>The corresponding entry of <strong>y</strong> is the estimated probability distribution for these parameters.</p></li>
</ul>
</div></blockquote>
</li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="generate_data.CRN_Dataset.samples_probs">
<span class="sig-name descname"><span class="pre">samples_probs</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/generate_data.html#CRN_Dataset.samples_probs"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#generate_data.CRN_Dataset.samples_probs" title="Permalink to this definition"></a></dt>
<dd><p>Runs <span class="math notranslate nohighlight">\(n_{\text{trajectories}}\)</span> of Stochastic Simulations for the parameters in input and estimates the 
corresponding distribution for the species indexed by <strong>ind_species</strong>.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>params</strong> (np.ndarray): Parameters associated to the propensity functions for each time window. Array of shape 
<span class="math notranslate nohighlight">\((L, M_{\theta}+M_{\xi})\)</span>.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p><strong>samples</strong>: List of the distributions for the corresponding species at sampling times. This list begins with time and 
parameters: <span class="math notranslate nohighlight">\([t, \theta_1, ..., \theta_{M_{\theta}}, \xi_1^1, \xi_1^2, ..., \xi_1^{M_{\xi}}, ..., \xi^{M_{\xi}}_L, p_0(t,\theta,\xi), ...]\)</span>.</p></li>
<li><p><strong>max_value</strong>: Maximum value reached during simulations <span class="math notranslate nohighlight">\(+ M_{\text{tot}} + 1\)</span> (for time). 
Used to standardise the length to turn the list of data vectors into an array.</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">list</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="generate_data.CRN_Dataset.set_length">
<span class="sig-name descname"><span class="pre">set_length</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">onedim_tab</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">length</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/generate_data.html#CRN_Dataset.set_length"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#generate_data.CRN_Dataset.set_length" title="Permalink to this definition"></a></dt>
<dd><p>Adds enough zeros at the end of an array to adjust its length.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>onedim_tab</strong> (np.ndarray): Array to extend. In this case, 1D array.</p></li>
<li><p><strong>length</strong> (int): Expected length of array.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>The array in input with zeros at the end so that its length equals <cite>length</cite>.</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="generate_data.CRN_Simulations">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">generate_data.</span></span><span class="sig-name descname"><span class="pre">CRN_Simulations</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">crn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_windows</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_trajectories</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ind_species</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'SSA'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">complete_trajectory</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sampling_times</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">array([],</span> <span class="pre">dtype=float64)</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/generate_data.html#CRN_Simulations"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#generate_data.CRN_Simulations" title="Permalink to this definition"></a></dt>
<dd><p>Class to run simulations over time and to estimate the abundance evolution of a species.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>crn</strong> (simulation.CRN): CRN to work on.</p></li>
<li><p><strong>time_windows</strong> (np.ndarray): Time windows during which all parameters are constant. Its form is <span class="math notranslate nohighlight">\([t_1, ..., t_T]\)</span>,
such that the time windows are <span class="math notranslate nohighlight">\([0, t_1], [t_1, t_2], ..., [t_{L-1}, t_L]\)</span>. <span class="math notranslate nohighlight">\(t_L\)</span> must match
with the final time <span class="math notranslate nohighlight">\(t_f\)</span>. If there is only one time window, it should be defined as <span class="math notranslate nohighlight">\([t_f]\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(n_{\text{trajectories}}\)</span> (int, optional): Number of trajectories to compute. Defaults to <span class="math notranslate nohighlight">\(10^4\)</span>.</p></li>
<li><p><strong>ind_species</strong> (int, optional): Index of the species of study. Defaults to <span class="math notranslate nohighlight">\(0\)</span>.</p></li>
<li><p><strong>method</strong> (str, optional): Stochastic Simulation to compute. Defaults to <cite>SSA</cite>.</p></li>
<li><p><strong>complete_trajectory</strong> (bool, optional): If True, computes the complete Jump Process. If False,
computes the abundance of the species to study at the specified sampling times. Defaults to True.</p></li>
<li><p><strong>sampling_times</strong> (np.ndarray, optional): Sampling times. Should not be specified when <strong>complete_trajectory</strong> is True.
Defaults to <cite>np.empty(0)</cite>.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="generate_data.CRN_Simulations.plot_simulations">
<span class="sig-name descname"><span class="pre">plot_simulations</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">params</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">targets</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(False,</span> <span class="pre">None)</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/generate_data.html#CRN_Simulations.plot_simulations"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#generate_data.CRN_Simulations.plot_simulations" title="Permalink to this definition"></a></dt>
<dd><p>Plots either all the simulated trajectories if <strong>complete_trajectory</strong> is False or 
the mean evolution of the abundance if <strong>complete_trajectory</strong> is True.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>params</strong> (np.ndarray): Parameters associated to the propensity functions for each time window. Array of shape 
<span class="math notranslate nohighlight">\((L, M_{\theta}+M_{\xi})\)</span>.</p></li>
<li><p><strong>targets</strong> (np.ndarray, optional): Target values. If None, no target value. Defaults to None.</p></li>
<li><p><strong>save</strong> (Tuple[bool, str], optional): If the first argument is True, saves the plot. The second argument 
is the name of the file under which to save the plot. Defaults to (False, None).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="generate_data.CRN_Simulations.run_simulations">
<span class="sig-name descname"><span class="pre">run_simulations</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/generate_data.html#CRN_Simulations.run_simulations"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#generate_data.CRN_Simulations.run_simulations" title="Permalink to this definition"></a></dt>
<dd><p>Runs <span class="math notranslate nohighlight">\(n_{\text{trajectories}}\)</span> of Stochastic Simulations for the parameters in input and deducts the 
corresponding distribution for the species indexed by <strong>ind_species</strong>.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>params</strong> (np.ndarray): Parameters associated to the propensity functions for each time window. Array of shape 
<span class="math notranslate nohighlight">\((L, M_{\theta}+M_{\xi})\)</span>.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><dl class="simple">
<dt>When <strong>complete_trajectory</strong> is True:</dt><dd><ul class="simple">
<li><p><strong>samples</strong> (dict): Each key is the index of the corresponding computed trajectory. Its value is an array with the abundance 
values after each jump.</p></li>
<li><p><strong>times</strong> (dict): Each key is the index of the corresponding computed trajectory. Its value is an array with the times at which
each jump occured.</p></li>
</ul>
</dd>
<dt>When <strong>complete_trajectory</strong> is False:</dt><dd><ul class="simple">
<li><p><strong>samples</strong> (np.ndarray): Measured abundance for each trajectory at the sampling times. Shape (n_trajectories, n_sampling_times).</p></li>
<li><p><strong>times</strong> (np.ndarray): Sampling times.</p></li>
</ul>
</dd>
</dl>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code>], <code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code>]]</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="saving-data">
<h3>Saving data<a class="headerlink" href="#saving-data" title="Permalink to this heading"></a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="convert_csv.array_to_csv">
<span class="sig-prename descclassname"><span class="pre">convert_csv.</span></span><span class="sig-name descname"><span class="pre">array_to_csv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">arr</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">file_name</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/convert_csv.html#array_to_csv"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#convert_csv.array_to_csv" title="Permalink to this definition"></a></dt>
<dd><p>Saves an array in a CSV file.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>arr</strong> (np.ndarray): Array to save.</p></li>
<li><p><strong>file_name</strong> (str): Name of the CSV file to create.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="convert_csv.csv_to_array">
<span class="sig-prename descclassname"><span class="pre">convert_csv.</span></span><span class="sig-name descname"><span class="pre">csv_to_array</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_name</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/convert_csv.html#csv_to_array"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#convert_csv.csv_to_array" title="Permalink to this definition"></a></dt>
<dd><p>Loads an array from a CSV file.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>file_name</strong> (str): Name of the CSV file to load.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>An array loaded from the CSV file.</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="convert_csv.csv_to_tensor">
<span class="sig-prename descclassname"><span class="pre">convert_csv.</span></span><span class="sig-name descname"><span class="pre">csv_to_tensor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_name</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/convert_csv.html#csv_to_tensor"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#convert_csv.csv_to_tensor" title="Permalink to this definition"></a></dt>
<dd><p>Loads an array from a CSV file.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>file_name</strong> (str): Name of the CSV file to load.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>A tensor loaded from the CSV file.</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="generate_csv.generate_csv_datasets">
<span class="sig-prename descclassname"><span class="pre">generate_csv.</span></span><span class="sig-name descname"><span class="pre">generate_csv_datasets</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">crn_name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">datasets</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_fixed_params</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_control_params</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stoich_mat</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">propensities</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_windows</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sampling_times</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ind_species</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_trajectories</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sobol_start</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sobol_end</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initial_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(False,</span> <span class="pre">None)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'SSA'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/generate_csv.html#generate_csv_datasets"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#generate_csv.generate_csv_datasets" title="Permalink to this definition"></a></dt>
<dd><p>Generates datasets from Stochastic Simulations and saves them in CSV files.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>crn_name</strong> (str): Name of the CRN to use for the files names.</p></li>
<li><p><strong>datasets</strong> (dict): Dictionary whose keys are the names of the datasets and whose values are the corresponding lengths.</p></li>
<li><p><strong>n_fixed_params</strong> (int): Number of fixed parameters required to define the propensity functions <span class="math notranslate nohighlight">\(M_{\theta}\)</span>.</p></li>
<li><p><strong>n_control_params</strong> (int): Number of control parameters required to define the propensity functions <span class="math notranslate nohighlight">\(M_{\xi}\)</span>.
Their values vary from a time window to another.</p></li>
<li><p><strong>stoich_mat</strong> (np.ndarray): Stoichiometry matrix.</p></li>
<li><p><strong>propensities</strong> (np.ndarray): Non-parameterised propensity functions.</p></li>
<li><p><strong>time_windows</strong> (np.ndarray): Time windows during which all parameters are constant. Its form is <span class="math notranslate nohighlight">\([t_1, ..., t_L]\)</span>,
such that the time windows are <span class="math notranslate nohighlight">\([0, t_1], [t_1, t_2], ..., [t_{L-1}, t_L]\)</span>. <span class="math notranslate nohighlight">\(t_L\)</span> must match
with the final time <span class="math notranslate nohighlight">\(t_f\)</span>. If there is only one time window, it should be defined as <span class="math notranslate nohighlight">\([t_f]\)</span>.</p></li>
<li><p><strong>sampling_times</strong> (np.ndarray): Sampling times.</p></li>
<li><p><strong>ind_species</strong> (int): Index of the species of interest.</p></li>
<li><p><span class="math notranslate nohighlight">\(n_{\text{trajectories}}\)</span> (int): Number of trajectories to compute to estimate the distribution for each set of parameters.</p></li>
<li><p><strong>sobol_start</strong> (np.ndarray): Lower boundaries of the parameters samples. Has shape <span class="math notranslate nohighlight">\((M_{\text{tot}},)\)</span>.</p></li>
<li><p><strong>sobol_end</strong> (np.ndarray): Upper boundaries of the parameters samples. Has shape <span class="math notranslate nohighlight">\((M_{\text{tot}},)\)</span>.</p></li>
<li><p><strong>initial_state</strong> (Tuple[bool, np.ndarray], optional): Initial state of the species. Defaults to (False, None), which
sets the initial state to <span class="math notranslate nohighlight">\(0\)</span> for all species.</p></li>
<li><p><strong>method</strong> (str): Stochastic Simulation to compute. Defaults to <cite>SSA</cite>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="generate_csv.generate_csv_simulations">
<span class="sig-prename descclassname"><span class="pre">generate_csv.</span></span><span class="sig-name descname"><span class="pre">generate_csv_simulations</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">crn_name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_fixed_params</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_control_params</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stoich_mat</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">propensities</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_windows</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sampling_times</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ind_species</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_trajectories</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">params</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initial_state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'SSA'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/generate_csv.html#generate_csv_simulations"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#generate_csv.generate_csv_simulations" title="Permalink to this definition"></a></dt>
<dd><p>Generates simulations of the abundance evolution of a species from Stochastic Simulations
and saves them in CSV files.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>crn_name</strong> (str): Name of the CRN to use for the file name.</p></li>
<li><p><strong>n_fixed_params</strong> (int): Number of fixed parameters required to define the propensity functions <span class="math notranslate nohighlight">\(M_{\theta}\)</span>.</p></li>
<li><p><strong>n_control_params</strong> (int): Number of control parameters required to define the propensity functions <span class="math notranslate nohighlight">\(M_{\xi}\)</span>.
Their values vary from a time window to another.</p></li>
<li><p><strong>stoich_mat</strong> (np.ndarray): Stoichiometry matrix.</p></li>
<li><p><strong>propensities</strong> (np.ndarray): Non-parameterised propensity functions.</p></li>
<li><p><strong>time_windows</strong> (np.ndarray): Time windows during which all parameters are constant. Its form is <span class="math notranslate nohighlight">\([t_1, ..., t_L]\)</span>,
such that the time windows are <span class="math notranslate nohighlight">\([0, t_1], [t_1, t_2], ..., [t_{L-1}, t_L]\)</span>. <span class="math notranslate nohighlight">\(t_L\)</span> must match
with the final time <span class="math notranslate nohighlight">\(t_f\)</span>. If there is only one time window, it should be defined as <span class="math notranslate nohighlight">\([t_f]\)</span>.</p></li>
<li><p><strong>sampling_times</strong> (np.ndarray): Sampling times.</p></li>
<li><p><strong>ind_species</strong> (int): Index of the species of interest.</p></li>
<li><p><span class="math notranslate nohighlight">\(n_{\text{trajectories}}\)</span> (int): Number of trajectories to compute to estimate the distribution for each set of parameters.</p></li>
<li><p><strong>params</strong> (np.ndarray): Parameters to use for the simulations.</p></li>
<li><p><strong>initial_state</strong> (Tuple[bool, np.ndarray], optional): Initial state of the species. Defaults to (False, None), which
sets the initial state to <span class="math notranslate nohighlight">\(0\)</span> for all species.</p></li>
<li><p><strong>method</strong> (str): Stochastic Simulation to compute. Defaults to <cite>SSA</cite>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="building-and-training-a-mixture-density-network">
<h2>Building and training a Mixture Density Network<a class="headerlink" href="#building-and-training-a-mixture-density-network" title="Permalink to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="neuralnetwork.NeuralNetwork">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neuralnetwork.</span></span><span class="sig-name descname"><span class="pre">NeuralNetwork</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_comps</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_params</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_hidden</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">128</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mixture</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'NB'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">print_info</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/neuralnetwork.html#NeuralNetwork"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neuralnetwork.NeuralNetwork" title="Permalink to this definition"></a></dt>
<dd><p>Class to build a Mixture Density Network.
Based on <span id="id2">[<a class="reference internal" href="sources.html#id4" title="Augustinas Sukys, Kaan Ocal, and Ramon Grima. Approximating solutions of the chemical master equation using neural networks. iScience., 25:105010, 2022.">SOG22</a>]</span>.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>n_comps</strong> (int): Number of components of the output mixture.</p></li>
<li><p><strong>n_params</strong> (int): Number of parameters in input <span class="math notranslate nohighlight">\(M_{\text{tot}}\)</span>, excluding the time parameter.</p></li>
<li><p><strong>n_hidden</strong> (int, optional): Number of neurons in the hidden layer. Defaults to <span class="math notranslate nohighlight">\(128\)</span>.</p></li>
<li><p><strong>mixture</strong> (str, optional): Type of mixture to compute. Defaults to <cite>NB</cite> for a Negative Binomial mixture.
Can also be <cite>Poisson</cite> for a Poisson mixture.</p></li>
<li><p><strong>print_info</strong> (bool, optional): If True, prints ‘Mixture Density Network created’ once the Neural Network is built. 
Defaults to True.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="neuralnetwork.NeuralNetwork.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/neuralnetwork.html#NeuralNetwork.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neuralnetwork.NeuralNetwork.forward" title="Permalink to this definition"></a></dt>
<dd><p>Runs the forward function of the Mixture Density Network.</p>
<dl>
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>input</strong> (torch.tensor): The input parameters to predict: 
<span class="math notranslate nohighlight">\([t, \theta_1, ..., \theta_{M_{\theta}}, \xi_1^1, \xi_1^2, ..., \xi_1^{M_{\xi}},\xi_2^1, ..., \xi^{M_{\xi}}_L]\)</span>.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul>
<li><dl>
<dt>A tuple of three tensors.</dt><dd><ul class="simple">
<li><p><strong>layer_ww</strong>: Mixture weights.</p></li>
</ul>
<dl class="simple">
<dt>For a Negative Binomial Mixture:</dt><dd><ul class="simple">
<li><p><strong>layer_rr</strong>: Count parameters.</p></li>
<li><p><strong>layer_pp</strong>: Success probabilities.</p></li>
</ul>
</dd>
<dt>For a Poisson Mixture:</dt><dd><ul class="simple">
<li><p><strong>layer_rr</strong>: Rate parameters.</p></li>
</ul>
</dd>
</dl>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">tensor</span></code>]</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="neuralnetwork.distr_pdf">
<span class="sig-prename descclassname"><span class="pre">neuralnetwork.</span></span><span class="sig-name descname"><span class="pre">distr_pdf</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">params</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mixture</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/neuralnetwork.html#distr_pdf"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neuralnetwork.distr_pdf" title="Permalink to this definition"></a></dt>
<dd><p>Computes the probability density function (pdf) of the components of the mixture.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>params</strong> (Tuple[torch.tensor]): Parameters needed to define the probability distribution.</p></li>
<li><p><strong>k</strong> (torch.tensor): Points at which to evaluate the pdf.</p></li>
<li><p><strong>mixture</strong> (str): Name of the chosen distribution for the mixture.</p></li>
<li><p><strong>eps</strong> (float, optional): Corrective term since a Negative Binomial cannot be evaluated at <span class="math notranslate nohighlight">\(p=1.0\)</span>.
Defaults to <span class="math notranslate nohighlight">\(10^{-5}\)</span>.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>The pdf of the distribution evaluated at <strong>k</strong>.</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="neuralnetwork.mix_pdf">
<span class="sig-prename descclassname"><span class="pre">neuralnetwork.</span></span><span class="sig-name descname"><span class="pre">mix_pdf</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">yy</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/neuralnetwork.html#mix_pdf"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neuralnetwork.mix_pdf" title="Permalink to this definition"></a></dt>
<dd><p>Computes the predicted distribution of the Neural Network at input points <strong>x</strong> and evaluates its pdf at points <strong>yy</strong>.</p>
<dl class="simple">
<dt>Parameters of the distribution mixture:</dt><dd><ul class="simple">
<li><p><strong>ww</strong>: Weights of the mixture.</p></li>
<li><p><strong>params</strong>: Parameters to define the distribution.</p></li>
</ul>
</dd>
</dl>
<p>A distribution mixture evaluated at point <strong>k</strong> is given by:</p>
<div class="math notranslate nohighlight">
\[q(k) = \sum_i w_i \text{Distr}(k, \text{params}_i)\]</div>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>model</strong> (NeuralNetwork): Mixture Density Network model.</p></li>
<li><p><strong>x</strong> (torch.tensor): Input points, 
<span class="math notranslate nohighlight">\([t, \theta_1, ..., \theta_{M_{\theta}}, \xi_1^1, \xi_1^2, ..., \xi_1^{M_{\xi}}, ..., \xi_L^{M_{\xi}}]\)</span>.</p></li>
<li><p><strong>yy</strong> (torch.tensor): Points at which to evaluate the pdf.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>The pdf of a mixture of distributions evaluated at <strong>k</strong>.</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="neuralnetwork.loss_kldivergence">
<span class="sig-prename descclassname"><span class="pre">neuralnetwork.</span></span><span class="sig-name descname"><span class="pre">loss_kldivergence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/neuralnetwork.html#loss_kldivergence"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neuralnetwork.loss_kldivergence" title="Permalink to this definition"></a></dt>
<dd><p>Computes the Kullback-Leibler divergence from the predicted distribution at input points <strong>x</strong> to the expected output <strong>y</strong>.</p>
<p>For tensors of the same shape <span class="math notranslate nohighlight">\(\hat{y}\)</span> and <span class="math notranslate nohighlight">\(y\)</span>:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[KL(y, \hat{y}) = y \log \bigl(\frac{y}{\hat{y}}\bigl)\]</div>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>x</strong> (torch.tensor): Vector of inputs.</p></li>
<li><p><strong>y</strong> (torch.tensor): Expected vector of outputs <span class="math notranslate nohighlight">\(y\)</span>.</p></li>
<li><p><strong>model</strong> (NeuralNetwork): Mixture Density Network model.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p><span class="math notranslate nohighlight">\(KL(y, \hat{y})\)</span>: Kullback-Leibler divergence between the predicted fistribution of the Neural Network at input
points <strong>x</strong> and the expected output <strong>y</strong>.</p></li>
</ul>
</dd>
</dl>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="neuralnetwork.loss_hellinger">
<span class="sig-prename descclassname"><span class="pre">neuralnetwork.</span></span><span class="sig-name descname"><span class="pre">loss_hellinger</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/neuralnetwork.html#loss_hellinger"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neuralnetwork.loss_hellinger" title="Permalink to this definition"></a></dt>
<dd><p>Computes the Hellinger distance from the predicted distribution at input points <strong>x</strong> to the expected output <strong>y</strong>.</p>
<p>For tensors of the same shape <span class="math notranslate nohighlight">\(\hat{y}\)</span> and <span class="math notranslate nohighlight">\(y\)</span>:</p>
<div class="math notranslate nohighlight">
\[H(y, \hat{y}) =\sqrt{1 - \sum_i \sqrt{\hat{y}_iy_i}}\]</div>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>x</strong> (torch.tensor): Vector of inputs.</p></li>
<li><p><strong>y</strong> (torch.tensor): Expected vector of outputs.</p></li>
<li><p><strong>model</strong> (NeuralNetwork): Mixture Density Network model.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p><span class="math notranslate nohighlight">\(H(y, \hat{y})\)</span>: Hellinger distance between the predicted distributions of the Neural Network at input points <strong>X</strong>
and the expected outputs <strong>y</strong>.</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="neuralnetwork.mean_loss">
<span class="sig-prename descclassname"><span class="pre">neuralnetwork.</span></span><span class="sig-name descname"><span class="pre">mean_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss=&lt;function</span> <span class="pre">loss_kldivergence&gt;</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/neuralnetwork.html#mean_loss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neuralnetwork.mean_loss" title="Permalink to this definition"></a></dt>
<dd><p>Computes the average loss over a batch.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>X</strong> (torch.tensor): Vector of inputs.</p></li>
<li><p><strong>y</strong> (torch.tensor): Expected vector of outputs.</p></li>
<li><p><strong>model</strong> (NeuralNetwork): Mixture Density Network model.</p></li>
<li><p><strong>loss</strong> (Callable, optional): Chosen loss. Defaults to <cite>loss_kldivergence</cite>.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>Average loss between the predicted distribution of the Neural Network at input points <strong>x</strong> and the expected output <strong>y</strong>.</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="neuralnetwork.NNTrainer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">neuralnetwork.</span></span><span class="sig-name descname"><span class="pre">NNTrainer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">valid_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr=0.005</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">l2_reg=0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_rounds=700</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batchsize=64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimiser=&lt;class</span> <span class="pre">'torch.optim.adam.Adam'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_early_stopping=(False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">1e-06)</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/neuralnetwork.html#NNTrainer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neuralnetwork.NNTrainer" title="Permalink to this definition"></a></dt>
<dd><p>Class to train the Mixture Density Network model.</p>
<p>Hyperparameters and parameters are saved in the dictionary <cite>args</cite>.</p>
<dl>
<dt>Args:</dt><dd><ul>
<li><p><strong>model</strong> (NeuralNetwork): Mixture Density Network model.</p></li>
<li><p><strong>train_data</strong> (Tuple[torch.tensor, torch.tensor]): Training dataset.</p></li>
<li><p><strong>valid_data</strong> (Tuple[torch.tensor, torch.tensor]): Validation dataset.</p></li>
<li><p><span class="math notranslate nohighlight">\(l_r\)</span> (float, optional): Initial learning rate. Defaults to <span class="math notranslate nohighlight">\(0.005\)</span>.</p></li>
<li><p><strong>l2_reg</strong> (float, optional): L2-regularisation term. Defaults to <span class="math notranslate nohighlight">\(0\)</span>.</p></li>
<li><p><strong>max_rounds</strong> (int, optional): Maximal number of epochs. Defaults to <span class="math notranslate nohighlight">\(700\)</span>.</p></li>
<li><p><strong>batchsize</strong> (int, optional): Number of elements in a batch. Defaults to <span class="math notranslate nohighlight">\(64\)</span>.</p></li>
<li><p><strong>optimiser</strong> (Callable, optional): Chosen optimiser. Defaults to torch.optim.Adam.</p></li>
<li><p><strong>add_early_stopping</strong> (Tuple[bool, int, float], optional): Defaults to (False, <span class="math notranslate nohighlight">\(50\)</span>, <span class="math notranslate nohighlight">\(10^{-6}\)</span>).</p>
<blockquote>
<div><ul class="simple">
<li><p>(bool): If True, uses the early stopping regularisation. Defaults to False.</p></li>
<li><p><strong>patience</strong> (int): Patience level <span class="math notranslate nohighlight">\(n_p\)</span>. 
At epoch <span class="math notranslate nohighlight">\(n\)</span>, the <span class="math notranslate nohighlight">\((n-n_p)\)</span> -th epoch is compared pairwise with that 
of the last <span class="math notranslate nohighlight">\(n_p\)</span> epochs. Defaults to <span class="math notranslate nohighlight">\(50\)</span>.</p></li>
<li><p><strong>delta</strong> (float): Tolerance threshold <span class="math notranslate nohighlight">\(\delta\)</span>. Training is stopped if the decrease between 
the elements of one of those pairs is lower than <span class="math notranslate nohighlight">\(\delta\)</span>. Defaults to <span class="math notranslate nohighlight">\(10^{-6}\)</span>.</p></li>
</ul>
</div></blockquote>
</li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="neuralnetwork.NNTrainer.early_stopping">
<span class="sig-name descname"><span class="pre">early_stopping</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/neuralnetwork.html#NNTrainer.early_stopping"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neuralnetwork.NNTrainer.early_stopping" title="Permalink to this definition"></a></dt>
<dd><p>Computes early stopping regularisation to avoid overfitting.
If <span class="math notranslate nohighlight">\(n_p\)</span> number of rounds pass without improving the validation loss by at least <span class="math notranslate nohighlight">\(\delta\)</span>,
stops the training.</p>
<p>Only called if <cite>add_early_stopping</cite> is True.</p>
<dl class="simple">
<dt>Returns:</dt><dd><ul class="simple">
<li><p>A boolean stating if the training should be stopped.</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="neuralnetwork.NNTrainer.update_losses">
<span class="sig-name descname"><span class="pre">update_losses</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/neuralnetwork.html#NNTrainer.update_losses"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neuralnetwork.NNTrainer.update_losses" title="Permalink to this definition"></a></dt>
<dd><p>Computes training and validation losses at each iteration and stores them.</p>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="neuralnetwork.train_round">
<span class="sig-prename descclassname"><span class="pre">neuralnetwork.</span></span><span class="sig-name descname"><span class="pre">train_round</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">trainer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss=&lt;function</span> <span class="pre">loss_kldivergence&gt;</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/neuralnetwork.html#train_round"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neuralnetwork.train_round" title="Permalink to this definition"></a></dt>
<dd><p>Performs one training epoch.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>trainer</strong> (NNTrainer): Training structure.</p></li>
<li><p><strong>loss</strong> (Callable, optional): Chosen loss for optimisation. Defaults to <cite>loss_kldivergence</cite>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="neuralnetwork.train_NN">
<span class="sig-prename descclassname"><span class="pre">neuralnetwork.</span></span><span class="sig-name descname"><span class="pre">train_NN</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">valid_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss=&lt;function</span> <span class="pre">loss_kldivergence&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">print_results=True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">print_info=True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">**kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/neuralnetwork.html#train_NN"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#neuralnetwork.train_NN" title="Permalink to this definition"></a></dt>
<dd><p>Trains the Neural Network.</p>
<dl>
<dt>Args:</dt><dd><ul>
<li><p><strong>model</strong> (NeuralNetwork): Mixture Density Network model to train.</p></li>
<li><p><strong>train_data</strong> (Tuple[torch.tensor, torch.tensor]): Training dataset.</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>X_train</strong>: Tensor of input data.</p></li>
<li><p><strong>y_train</strong>: Tensor of expected outputs.</p></li>
</ul>
</div></blockquote>
</li>
<li><p><strong>valid_data</strong> (Tuple[torch.tensor, torch.tensor]): Validation dataset. Only used for early stopping.</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>X_valid</strong>: Tensor of input data.</p></li>
<li><p><strong>y_valid</strong>: Tensor of expected outputs.</p></li>
</ul>
</div></blockquote>
</li>
<li><p><strong>loss</strong> (Callable, optional): Chosen loss for optimisation. Defaults to <cite>loss_kldivergence</cite>.</p></li>
<li><p><strong>print_results</strong> (bool, optional): If True, prints the final results
(final learning rate, train and valid losses). Defaults to True.</p></li>
<li><p><strong>print_info</strong> (bool, optional): If True, prints a progress bar. Defaults to True.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>Training and validation losses for each epoch.</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">list</span></code>]</p>
</dd>
</dl>
</dd></dl>

<p>The example file <code class="docutils literal notranslate"><span class="pre">training.py</span></code> trains a Mixture Density Network model on the Production and Degradation Chemical Reaction Network using these functions.</p>
</section>
<section id="computing-probability-mass-functions-and-the-sensitivity-of-the-likelihood">
<h2>Computing probability mass functions and the sensitivity of the likelihood<a class="headerlink" href="#computing-probability-mass-functions-and-the-sensitivity-of-the-likelihood" title="Permalink to this heading"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="get_sensitivities.probabilities">
<span class="sig-prename descclassname"><span class="pre">get_sensitivities.</span></span><span class="sig-name descname"><span class="pre">probabilities</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">length_output</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">200</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/get_sensitivities.html#probabilities"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#get_sensitivities.probabilities" title="Permalink to this definition"></a></dt>
<dd><p>Computes the probability mass functions for the <span class="math notranslate nohighlight">\(N_{\max}\)</span> first elements.
Output has shape <span class="math notranslate nohighlight">\((N_{\max},)\)</span>.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>inputs</strong> (torch.tensor): Input data.</p></li>
<li><p><strong>model</strong> (neuralnetwork.NeuralNetwork): Mixture Density Network model.</p></li>
<li><p><strong>length_output</strong> (int, optional): Length of the output <span class="math notranslate nohighlight">\(N_{\max}\)</span>. Defaults to <span class="math notranslate nohighlight">\(200\)</span>.</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="get_sensitivities.sensitivities">
<span class="sig-prename descclassname"><span class="pre">get_sensitivities.</span></span><span class="sig-name descname"><span class="pre">sensitivities</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">length_output</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">200</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">with_probs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/get_sensitivities.html#sensitivities"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#get_sensitivities.sensitivities" title="Permalink to this definition"></a></dt>
<dd><p>Computes the gradient of the probability mass functions with respect to the time and to the input parameters.
Output has shape <span class="math notranslate nohighlight">\((N_{\max}, 1 + M_{\text{tot}})\)</span>.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>inputs</strong> (torch.tensor): Input data.</p></li>
<li><p><strong>model</strong> (neuralnetwork.NeuralNetwork): Mixture Density Network model.</p></li>
<li><p><strong>length_output</strong> (int, optional): Length of the output <span class="math notranslate nohighlight">\(N_{\max}\)</span>. Defaults to <span class="math notranslate nohighlight">\(200\)</span>.</p></li>
<li><p><strong>with_probs</strong> (bool, optional): If True, also returns the corresponding probability distribution. Defaults to False.</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">tensor</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">tensor</span></code>]]</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="computing-the-fisher-information">
<h2>Computing the Fisher Information<a class="headerlink" href="#computing-the-fisher-information" title="Permalink to this heading"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="get_fi.fisher_information_t">
<span class="sig-prename descclassname"><span class="pre">get_fi.</span></span><span class="sig-name descname"><span class="pre">fisher_information_t</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">probs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sensitivities</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/get_fi.html#fisher_information_t"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#get_fi.fisher_information_t" title="Permalink to this definition"></a></dt>
<dd><p>Computes the Fisher Information of a marginal probability mass function at a single time point.</p>
<p>In the case of a finite state-space, we can enumerate its elements as <span class="math notranslate nohighlight">\(\{ x_t^1, x_t^2, ..., x_t^{N_{\max}} \}\)</span>. We then have:</p>
<div class="math notranslate nohighlight">
\[\forall (i,j) \in [\![1, M_{\text{tot}}]\!]^2, 
[\mathcal{I}_t^{\theta,\xi}]_{ij} = \sum_{\ell=1}^{N_{\max}} \frac{1}{p_\ell(t,\theta,\xi)}[S_t^{\theta,\xi}]_{\ell i}[S_t^{\theta,\xi}]_{\ell j}\]</div>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>probs</strong> (np.ndarray): The probability vector. Has shape <span class="math notranslate nohighlight">\((N_{\max},)\)</span>.</p></li>
<li><p><strong>sensitivities</strong> (np.ndarray): The sensitivity matrix. Has shape <span class="math notranslate nohighlight">\((N_{\max}, M_{\text{tot}})\)</span>.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>The Fisher Information estimated by the model at a single time point. Has shape <span class="math notranslate nohighlight">\((M_{\text{tot}}, M_{\text{tot}})\)</span>.</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="get_fi.fisher_information">
<span class="sig-prename descclassname"><span class="pre">get_fi.</span></span><span class="sig-name descname"><span class="pre">fisher_information</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_sampling_times</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">probs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sensitivities</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/get_fi.html#fisher_information"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#get_fi.fisher_information" title="Permalink to this definition"></a></dt>
<dd><p>Computes the Fisher Information Matrix at different time points.</p>
<p>As defined in <span id="id3">[<a class="reference internal" href="sources.html#id2" title="Zachary R Fox and Brian Munsky. The finite state projection based fisher information matrix approach to estimate information and optimize single-cell experiments. PLoS Computational Biology., 15:e1006365, 2019.">FM19</a>]</span>:</p>
<div class="math notranslate nohighlight">
\[\forall (i,j) \in [\![1, M_{\text{tot}}]\!]^2, 
[\mathcal{I}_t^\theta]_{ij} = \sum_{k=1}^L [\mathcal{I}_{t_k}^{\theta,\xi}]_{ij} \sum_{\ell=1}^{N_{\max}} 
\frac{1}{p_\ell(t_k,\theta,\xi)}[S_{t_k}^{\theta,\xi}]_{\ell i}[S_{t_k}^{\theta,\xi}]_{\ell j}\]</div>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>n_sampling_times</strong> (int): Number of sampling times.</p></li>
<li><p><strong>probs</strong> (np.ndarray): The probability vector. Has shape (<strong>n_sampling_times</strong>, <span class="math notranslate nohighlight">\(N_{\max}\)</span>).</p></li>
<li><p><strong>sensitivities</strong> (np.ndarray): The sensitivity of the likelihood matrix. Has shape (<strong>n_sampling_times</strong>, <span class="math notranslate nohighlight">\(N_{\max}\)</span>, <span class="math notranslate nohighlight">\(M_{\text{tot}}\)</span>).</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>The Fisher Information estimated by the model. Has shape <span class="math notranslate nohighlight">\((M_{\text{tot}}, M_{\text{tot}})\)</span>.</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="computing-the-expectation-and-its-gradient">
<h2>Computing the expectation and its gradient<a class="headerlink" href="#computing-the-expectation-and-its-gradient" title="Permalink to this heading"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="get_sensitivities.expected_val">
<span class="sig-prename descclassname"><span class="pre">get_sensitivities.</span></span><span class="sig-name descname"><span class="pre">expected_val</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss=&lt;function</span> <span class="pre">identity&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">length_output=200</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">array=True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/get_sensitivities.html#expected_val"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#get_sensitivities.expected_val" title="Permalink to this definition"></a></dt>
<dd><p>Computes the expectation of the probability mass function estimated by the MDN:</p>
<div class="math notranslate nohighlight">
\[E_{\theta,\xi}[X_t] = \sum_{k=1}^{N_{\max}} k \ p(k;t,\theta,\xi)\]</div>
<p>It can also compute <span class="math notranslate nohighlight">\(\mathcal{L}\big(E_{\theta,\xi}[X_t]\big)\)</span> where <span class="math notranslate nohighlight">\(\mathcal{L}\)</span>
is a given function.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>inputs</strong> (torch.tensor): Input data in the form requested by the MDN model: 
<span class="math notranslate nohighlight">\([t, \theta_1, ..., \theta_{M_{\theta}}, \xi_1^1, \xi_1^2, ..., \xi_1^{M_{\xi}}, \xi_2^1, ..., \xi_L^{M_{\xi}}]\)</span>.</p></li>
<li><p><strong>model</strong> (neuralnetwork.NeuralNetwork): Mixture Density Network model.</p></li>
<li><p><strong>loss</strong> (Callable, optional): Loss function. Must be compatible with PyTorch. Defaults to the <cite>identity</cite> function.</p></li>
<li><p><strong>length_output</strong> (int, optional): Upper bound of the truncated sum <span class="math notranslate nohighlight">\(N_{\max}\)</span>. Defaults to <span class="math notranslate nohighlight">\(200\)</span>.</p></li>
<li><p><strong>array</strong> (bool, optional): If True, the output type is a NumPy array. If False, it is a PyTorch tensor. Defaults to True.</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">tensor</span></code>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="get_sensitivities.gradient_expected_val">
<span class="sig-prename descclassname"><span class="pre">get_sensitivities.</span></span><span class="sig-name descname"><span class="pre">gradient_expected_val</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss=&lt;function</span> <span class="pre">identity&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">length_output=200</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/get_sensitivities.html#gradient_expected_val"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#get_sensitivities.gradient_expected_val" title="Permalink to this definition"></a></dt>
<dd><p>Computes the gradient of the expectation estimated by the MDN:</p>
<div class="math notranslate nohighlight">
\[\nabla_{t, \theta, \xi} E_{\theta, \xi}[X_t] = \sum_{k=1}^{N_{\max}} k \ \nabla_{t, \theta, \xi}p(k;t,\theta,\xi)\]</div>
<p>It can also compute <span class="math notranslate nohighlight">\(\nabla_{t, \theta, \xi} \mathcal{L}\big(E_{\theta,\xi}[X_t]\big)\)</span> where <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> is a given function.
Output has shape <span class="math notranslate nohighlight">\((1 + M_{\text{tot}})\)</span>.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>inputs</strong> (torch.tensor): Input data.</p></li>
<li><p><strong>model</strong> (neuralnetwork.NeuralNetwork): Mixture Density Network model.</p></li>
<li><p><strong>loss</strong> (Callable, optional): Loss function. Must be compatible with PyTorch. Defaults to the <cite>identity</cite> function.</p></li>
<li><p><strong>length_output</strong> (int, optional): Upper bound of the truncated sum <span class="math notranslate nohighlight">\(N_{\max}\)</span>. Defaults to <span class="math notranslate nohighlight">\(200\)</span>.</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="performing-a-projected-gradient-descent">
<h2>Performing a Projected Gradient Descent<a class="headerlink" href="#performing-a-projected-gradient-descent" title="Permalink to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="projected_gradient_descent.ProjectedGradientDescent">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">projected_gradient_descent.</span></span><span class="sig-name descname"><span class="pre">ProjectedGradientDescent</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">grad_loss</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">domain</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/projected_gradient_descent.html#ProjectedGradientDescent"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#projected_gradient_descent.ProjectedGradientDescent" title="Permalink to this definition"></a></dt>
<dd><p>Class to compute the Projected Gradient Descent Algorithm.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>grad_loss</strong> (Callable): Gradient function of the loss.</p></li>
<li><p><strong>domain</strong> (np.ndarray): Boundaries of the domain in which to project. Has shape <span class="math notranslate nohighlight">\((\text{dim}, 2)\)</span>.
<strong>domain[:,0]</strong> defines the lower boundaries for each dimension, <strong>domain[:,1]</strong> defines the 
upper boundaries for each dimension.</p></li>
<li><p><strong>dim</strong> (int): Dimension of the projection space.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="projected_gradient_descent.ProjectedGradientDescent.projected_gradient_descent">
<span class="sig-name descname"><span class="pre">projected_gradient_descent</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">init</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">20000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clipping_value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">50.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">progress_bar</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/projected_gradient_descent.html#ProjectedGradientDescent.projected_gradient_descent"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#projected_gradient_descent.ProjectedGradientDescent.projected_gradient_descent" title="Permalink to this definition"></a></dt>
<dd><p>Computes the Projected Gradient Descent.</p>
<div class="math notranslate nohighlight">
\[\xi_{n+1} \leftarrow \xi_n - \gamma \nabla_{\xi} C_{\xi_n}^J\]</div>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>init</strong> (np.ndarray): Initial values of the control parameters. Has shape <span class="math notranslate nohighlight">\((M_{\xi}\times L,)\)</span>.</p></li>
<li><p><strong>gamma</strong> (float): Step size <span class="math notranslate nohighlight">\(\gamma\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(n_{\text{iter}}\)</span> (int, optional): Maximal number of iterations allowed for the gradient descent. 
Defaults to <span class="math notranslate nohighlight">\(20 000\)</span>.</p></li>
<li><p><strong>eps</strong> (float, optional): Threshold level <span class="math notranslate nohighlight">\(\varepsilon\)</span>. The algorithm halts when the squared norm of the gradient of 
the loss value is smaller than <span class="math notranslate nohighlight">\(\varepsilon\)</span>. Defaults to <span class="math notranslate nohighlight">\(10^{-5}\)</span>.</p></li>
<li><p><strong>min_loss</strong> (float, optional): Minimal loss value. The algorithm halts when the loss value is smaller than <strong>min_loss</strong>.
Defaults to <span class="math notranslate nohighlight">\(-0.1\)</span>, which implies that this condition is never reached.</p></li>
<li><p><strong>clipping_value</strong> (float, optional): Maximal gradient norm value. Used to avoid explosing gradients.
Defaults to <span class="math notranslate nohighlight">\(50\)</span>.</p></li>
<li><p><strong>progress_bar</strong> (bool, optional): If True, plots a progress bar during the optimisation process. Defaults to True.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>Array of the control parameters values estimated at each iteration. Has shape <span class="math notranslate nohighlight">\((n, \text{dim})\)</span>.</p></li>
<li><p>Array of the loss values estimated at each iteration. Has shape <span class="math notranslate nohighlight">\((n,)\)</span>.</p></li>
<li><p>Array of the gradient values estimated at each iteration. Has shape <span class="math notranslate nohighlight">\((n, \text{dim})\)</span>.</p></li>
<li><p>Actual number of iterations <span class="math notranslate nohighlight">\(n\)</span> performed by the algorithm.</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="projected_gradient_descent.ProjectedGradientDescent_CRN">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">projected_gradient_descent.</span></span><span class="sig-name descname"><span class="pre">ProjectedGradientDescent_CRN</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">crn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">domain</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fixed_params</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_windows</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/projected_gradient_descent.html#ProjectedGradientDescent_CRN"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#projected_gradient_descent.ProjectedGradientDescent_CRN" title="Permalink to this definition"></a></dt>
<dd><p>Class to compute a Projected Gradient Descent for a CRN.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>crn</strong> (simulation.CRN): CRN to compute the PGD on.</p></li>
<li><p><strong>domain</strong> (np.ndarray): Boundaries of the domain in which to project. Has shape <span class="math notranslate nohighlight">\((\text{dim}, 2)\)</span>.
<strong>domain[:,0]</strong> defines the lower boundaries for each dimension, <strong>domain[:,1]</strong> defines the 
upper boundaries for each dimension.</p></li>
<li><p><strong>fixed_params</strong> (np.ndarray): Selected values for the fixed parameters.</p></li>
<li><p><strong>time_windows</strong> (np.ndarray): Time windows during which the parameters do not vary.
Its form is <span class="math notranslate nohighlight">\([t_1, ..., t_L]\)</span>, such that the time windows are 
<span class="math notranslate nohighlight">\([0, t_1], [t_1, t_2], ..., [t_{L-1}, t_L]\)</span>. <span class="math notranslate nohighlight">\(t_L\)</span> must match with the final time 
<span class="math notranslate nohighlight">\(t_f\)</span>. If there is only one time window, <strong>time_windows</strong> should be defined as <span class="math notranslate nohighlight">\([t_f]\)</span>.</p></li>
<li><p><strong>loss</strong> (Union[Callable, np.ndarray]): Loss function(s) used for the gradient descent. Either a single
function for all time windows, or an array of functions of shape <span class="math notranslate nohighlight">\((L,)\)</span>, each function corresponding to one time window.</p></li>
<li><p><strong>weights</strong> (np.ndarray, optional): Weights of each target. Has shape <span class="math notranslate nohighlight">\((L,)\)</span>. If None, all targets
have the same weight. Defaults to None.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="projected_gradient_descent.ProjectedGradientDescent_CRN.optimisation">
<span class="sig-name descname"><span class="pre">optimisation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">gamma</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-0.1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/projected_gradient_descent.html#ProjectedGradientDescent_CRN.optimisation"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#projected_gradient_descent.ProjectedGradientDescent_CRN.optimisation" title="Permalink to this definition"></a></dt>
<dd><p>Computes the Projected Gradient Descent algorithm.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>gamma</strong> (float): Step size <span class="math notranslate nohighlight">\(\gamma\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(n_{\text{iter}}\)</span> (int, optional): Number of iterations for the gradient descent. Defaults to <span class="math notranslate nohighlight">\(1000\)</span>.</p></li>
<li><p><strong>eps</strong> (int, optional): Tolerance rate <span class="math notranslate nohighlight">\(\varepsilon\)</span>. Defaults to <span class="math notranslate nohighlight">\(10^{-3}\)</span>.</p></li>
<li><p><strong>min_loss</strong> (float, optional): Minimal loss value. The algorithm halts when the loss value is smaller than <strong>min_loss</strong>.
Defaults to <span class="math notranslate nohighlight">\(-0.1\)</span>, which implies that this condition is never reached.</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="projected_gradient_descent.ProjectedGradientDescent_CRN.plot_abundances">
<span class="sig-name descname"><span class="pre">plot_abundances</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ind_species</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">targets</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(False,</span> <span class="pre">None)</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/projected_gradient_descent.html#ProjectedGradientDescent_CRN.plot_abundances"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#projected_gradient_descent.ProjectedGradientDescent_CRN.plot_abundances" title="Permalink to this definition"></a></dt>
<dd><p>Plots the mean evolution of the abundance over time based on the SSA method.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>ind_species</strong> (int): Index of the species of interest.</p></li>
<li><p><strong>targets</strong> (np.ndarray, optional): Target values at each time point.
If None, no target is plotted. Defaults to None.</p></li>
<li><p><strong>rate</strong> (int, optional): Plotting rate. Defaults to <span class="math notranslate nohighlight">\(1000\)</span>.</p></li>
<li><p><strong>save</strong> (Tuple[bool, str], optional): If the first argument is True, saves the file. The second argument 
is the name of the file under which to save the plot. Defaults to (False, None).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="projected_gradient_descent.ProjectedGradientDescent_CRN.plot_control_params_trajectory">
<span class="sig-name descname"><span class="pre">plot_control_params_trajectory</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">save</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(False,</span> <span class="pre">None)</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/projected_gradient_descent.html#ProjectedGradientDescent_CRN.plot_control_params_trajectory"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#projected_gradient_descent.ProjectedGradientDescent_CRN.plot_control_params_trajectory" title="Permalink to this definition"></a></dt>
<dd><p>Plots the values of the parameters <span class="math notranslate nohighlight">\(\xi\)</span> at each iteration.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>save</strong> (Tuple[bool, str], optional): If the first argument is True, saves the file. The second argument 
is the name of the file under which to save the plot. Defaults to (False, None).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="projected_gradient_descent.ProjectedGradientDescent_CRN.plot_control_values">
<span class="sig-name descname"><span class="pre">plot_control_values</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">save</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(False,</span> <span class="pre">None)</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/projected_gradient_descent.html#ProjectedGradientDescent_CRN.plot_control_values"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#projected_gradient_descent.ProjectedGradientDescent_CRN.plot_control_values" title="Permalink to this definition"></a></dt>
<dd><p>Plots the final values of the parameters <span class="math notranslate nohighlight">\(\xi\)</span> over time.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>save</strong> (Tuple[bool, str], optional): If the first argument is True, saves the file. The second argument 
then is the name of the file under which to save the plot. Defaults to (False, None).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="projected_gradient_descent.ProjectedGradientDescent_CRN.plot_gradients_trajectory">
<span class="sig-name descname"><span class="pre">plot_gradients_trajectory</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">save</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(False,</span> <span class="pre">None)</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/projected_gradient_descent.html#ProjectedGradientDescent_CRN.plot_gradients_trajectory"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#projected_gradient_descent.ProjectedGradientDescent_CRN.plot_gradients_trajectory" title="Permalink to this definition"></a></dt>
<dd><p>Plots the values of the gradients at each iteration.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>save</strong> (Tuple[bool, str], optional): If the first argument is True, saves the file. The second argument 
is the name of the file under which to save the plot. Defaults to (False, None).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="projected_gradient_descent.ProjectedGradientDescent_CRN.plot_losses_trajectory">
<span class="sig-name descname"><span class="pre">plot_losses_trajectory</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">save</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(False,</span> <span class="pre">None)</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/projected_gradient_descent.html#ProjectedGradientDescent_CRN.plot_losses_trajectory"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#projected_gradient_descent.ProjectedGradientDescent_CRN.plot_losses_trajectory" title="Permalink to this definition"></a></dt>
<dd><p>Plots the loss values over the iterations as estimated by the chosen model.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>save</strong> (Tuple[bool, str], optional): If the first argument is True, saves the file. The second argument 
then is the name of the file under which to save the plot. Defaults to (False, None).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="projected_gradient_descent.ProjectedGradientDescent_CRN.plot_performance_index">
<span class="sig-name descname"><span class="pre">plot_performance_index</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ind_species</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">200</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(False,</span> <span class="pre">None)</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/projected_gradient_descent.html#ProjectedGradientDescent_CRN.plot_performance_index"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#projected_gradient_descent.ProjectedGradientDescent_CRN.plot_performance_index" title="Permalink to this definition"></a></dt>
<dd><p>Plots the values of the losses over the iterations estimated by the SSA method.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>ind_species</strong> (int): Index of the species of interest.</p></li>
<li><p><strong>rate</strong> (int, optional): Plotting rate. Defaults to <span class="math notranslate nohighlight">\(200\)</span>.</p></li>
<li><p><strong>save</strong> (Tuple[bool, str], optional): If the first argument is True, saves the file. The second argument 
is the name of the file under which to save the plot. Defaults to (False, None).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="projected_gradient_descent.ProjectedGradientDescent_MDN">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">projected_gradient_descent.</span></span><span class="sig-name descname"><span class="pre">ProjectedGradientDescent_MDN</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">crn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">domain</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fixed_params</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_windows</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">length_output</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">200</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">with_correction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/projected_gradient_descent.html#ProjectedGradientDescent_MDN"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#projected_gradient_descent.ProjectedGradientDescent_MDN" title="Permalink to this definition"></a></dt>
<dd><p>Class to compute the Projected Gradient Descent (PGD) based on a Mixture Density Network model.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>crn</strong> (simulation.CRN): CRN to compute the PGD on.</p></li>
<li><p><strong>model</strong> (neuralnetwork.NeuralNetwork): MDN model used for the gradient descent.</p></li>
<li><p><strong>domain</strong> (np.ndarray): Boundaries of the domain in which to project. Has shape <span class="math notranslate nohighlight">\((\text{dim}, 2)\)</span>.
<strong>domain[:,0]</strong> defines the lower boundaries for each dimension, <strong>domain[:,1]</strong> defines the 
upper boundaries for each dimension.</p></li>
<li><p><strong>fixed_params</strong> (np.ndarray): Selected values for the fixed parameters <span class="math notranslate nohighlight">\(\theta\)</span>.</p></li>
<li><p><strong>time_windows</strong> (np.ndarray): Time windows during which the parameters do not vary. 
Its form is <span class="math notranslate nohighlight">\([t_1, ..., t_L]\)</span>, such that the time windows are 
<span class="math notranslate nohighlight">\([0, t_1], [t_1, t_2], ..., [t_{L-1}, t_L]\)</span>. <span class="math notranslate nohighlight">\(t_L\)</span> must match with the final time 
<span class="math notranslate nohighlight">\(t_f\)</span>. If there is only one time window, <strong>time_windows</strong> should be defined as <span class="math notranslate nohighlight">\([t_f]\)</span>.</p></li>
<li><p><strong>loss</strong> (Union[Callable, list]): Loss function used for the gradient descent. If it is a list, each element
is the loss function for the corresponding time window.</p></li>
<li><p><strong>weights</strong> (np.ndarray, optional): Weights of each target. Has shape <span class="math notranslate nohighlight">\((L,)\)</span>. If None, all targets
have the same weight. Defaults to None.</p></li>
<li><p><strong>length_output</strong> (int, optional): Length of the output of the MDN prediction. Defaults to <span class="math notranslate nohighlight">\(200\)</span>.</p></li>
<li><p><strong>with_correction</strong> (bool, optional): If True, sensitivities are set to zero when control parameters
have no influence on that time window. If False, works with the computed sensitivities untouched. Defaults to True.
(<span class="math notranslate nohighlight">\(\forall i \in [\![1, L]\!], \forall j \in [\![i=1, L]\!], \forall k \in [\![1, M_{\xi}]\!], [\hat{S}_{t_i}^{\theta, \xi}]_{.M+M_{\xi}\times j+k} = 0\)</span>).</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="projected_gradient_descent.ProjectedGradientDescent_MDN.create_gradient">
<span class="sig-name descname"><span class="pre">create_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">length_output</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">with_correction</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/projected_gradient_descent.html#ProjectedGradientDescent_MDN.create_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#projected_gradient_descent.ProjectedGradientDescent_MDN.create_gradient" title="Permalink to this definition"></a></dt>
<dd><p>Computes the gradient function of the loss evaluated at the expected value with respect to 
all control parameters.</p>
<div class="math notranslate nohighlight">
\[\xi \mapsto \sum_{i=1}^L w_i \nabla_{\xi} \mathcal{L}_i(E[X_t^{\theta, \xi}])\]</div>
<p>Usually, we set <span class="math notranslate nohighlight">\(\mathcal{L}_i\)</span> of the form <span class="math notranslate nohighlight">\(\mathcal{L}_i : x \mapsto (x-h_i)^2\)</span> 
where <span class="math notranslate nohighlight">\(h \in \mathbb{R}^L\)</span> is the target vector.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>length_output</strong> (int): Upper bound of the truncated expectation <span class="math notranslate nohighlight">\(N_{\max}\)</span>.</p></li>
<li><p><strong>with_correction</strong> (bool): If True, sensitivities are set to zero when control parameters have no influence 
on that time window. If False, works with the computed sensitivities untouched. Defaults to True.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="projected_gradient_descent.ProjectedGradientDescent_MDN.create_loss">
<span class="sig-name descname"><span class="pre">create_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">length_output</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/projected_gradient_descent.html#ProjectedGradientDescent_MDN.create_loss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#projected_gradient_descent.ProjectedGradientDescent_MDN.create_loss" title="Permalink to this definition"></a></dt>
<dd><p>Computes the loss function evaluated at the expected value.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>length_output</strong> (int): Upper bound of the truncated expected value <span class="math notranslate nohighlight">\(N_{\max}\)</span>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="projected_gradient_descent.control_method">
<span class="sig-prename descclassname"><span class="pre">projected_gradient_descent.</span></span><span class="sig-name descname"><span class="pre">control_method</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimiser</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_iter</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ind_species</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">targets</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">plot_performance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(True,</span> <span class="pre">['control_values',</span> <span class="pre">'experimental_losses',</span> <span class="pre">'parameters',</span> <span class="pre">'gradients_losses',</span> <span class="pre">'real_losses',</span> <span class="pre">'exp_results'])</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/projected_gradient_descent.html#control_method"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#projected_gradient_descent.control_method" title="Permalink to this definition"></a></dt>
<dd><p>Computes the PGD, plots the results and saves the algorithm and results data.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>optimiser</strong> (ProjectedGradientDescent_CRN): Either of type <code class="docutils literal notranslate"><span class="pre">ProjectedGradientDescent_MDN</span></code> or
<code class="docutils literal notranslate"><span class="pre">ProjectedGradientDescent_FSP</span></code>.</p></li>
<li><p><strong>gamma</strong> (float): Step size <span class="math notranslate nohighlight">\(\gamma\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(n_{\text{iter}}\)</span> (int): Maximal number of iterations allowed for the gradient descent.</p></li>
<li><p><strong>eps</strong> (float): Threshold level <span class="math notranslate nohighlight">\(\varepsilon\)</span>. The algorithm halts when the gradient of the loss value is smaller than <span class="math notranslate nohighlight">\(\varepsilon\)</span>.</p></li>
<li><p><strong>ind_species</strong> (int): Index of the species of interest.</p></li>
<li><p><strong>targets</strong> (np.ndarray): Target values at each time point. If None, no target is plotted.</p></li>
<li><p><strong>min_loss</strong> (float, optional): Minimal loss value. The algorithm halts when the loss value is smaller than <strong>min_loss</strong>.
Defaults to <span class="math notranslate nohighlight">\(-0.1\)</span>, which implies that this condition is never reached.</p></li>
<li><p><strong>plot_performance</strong> (bool, optional): If True, calls the function <code class="docutils literal notranslate"><span class="pre">optimiser.plot_performance_index</span></code>. Defaults to True.</p></li>
<li><p><strong>save</strong> (Tuple[bool, list], optional): If the first argument is True, saves the plots. The second argument 
is a list of the names of the files under which to save the plots. 
Defaults to (True, [“control_values”, “experimental_losses”, “parameters”, “gradients_losses”, “real_losses”, “exp_results”]).</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>Running time of the algorithm.</p></li>
<li><p>Final values for the control parameters.</p></li>
<li><p>Final loss value.</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="training_pgd.pgdMDN">
<span class="sig-prename descclassname"><span class="pre">training_pgd.</span></span><span class="sig-name descname"><span class="pre">pgdMDN</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">crn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ind_species</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">domain</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fixed_params</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_windows</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_iter</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">targets</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">crn_name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">directory</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(True,</span> <span class="pre">['control_values',</span> <span class="pre">'experimental_losses',</span> <span class="pre">'parameters',</span> <span class="pre">'gradients_losses',</span> <span class="pre">'real_losses',</span> <span class="pre">'exp_results'])</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/training_pgd.html#pgdMDN"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#training_pgd.pgdMDN" title="Permalink to this definition"></a></dt>
<dd><p>Performs the PGD using a MDN, saves the selected parameters for the algorithm and the results in a <cite>.txt</cite> file,
plots the results and saves them in CSV files.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>crn</strong> (simulation.CRN): CRN to work on. Make sure to specify the derivatives of the propensities if it does not follow
mass-action kinetics.</p></li>
<li><p><strong>model</strong> (neuralnetwork.NeuralNetwork): MDN model to use for the PGD.</p></li>
<li><p><strong>ind_species</strong> (int): Index of the species of interest.</p></li>
<li><p><strong>domain</strong> (np.ndarray): Boundaries of the domain in which to project. Has shape <span class="math notranslate nohighlight">\((\text{dim}, 2)\)</span>.
<strong>domain[:,0]</strong> defines the lower boundaries for each dimension, <strong>domain[:,1]</strong> defines the 
upper boundaries for each dimension.</p></li>
<li><p><strong>fixed_params</strong> (np.ndarray): Selected values for the fixed parameters <span class="math notranslate nohighlight">\(\theta\)</span>.</p></li>
<li><p><strong>time_windows</strong> (np.ndarray): Time windows during which all parameters are fixed. 
Its form is <span class="math notranslate nohighlight">\([t_1, ..., t_L]\)</span>, such that the time windows are 
<span class="math notranslate nohighlight">\([0, t_1], [t_1, t_2], ..., [t_{L-1}, t_L]\)</span>. <span class="math notranslate nohighlight">\(t_L\)</span> must match with the final time 
<span class="math notranslate nohighlight">\(t_f\)</span>. If there is only one time window, <strong>time_windows</strong> should be defined as <span class="math notranslate nohighlight">\([t_f]\)</span>.</p></li>
<li><p><strong>loss</strong> (Union[Callable, list]): Loss function used for the gradient descent. If it is a list, each element
is the loss function for the corresponding time window.</p></li>
<li><p><strong>gamma</strong> (float): Step size <span class="math notranslate nohighlight">\(\gamma\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(n_{\text{iter}}\)</span> (int): Maximal number of iterations allowed for the gradient descent.</p></li>
<li><p><strong>eps</strong> (float): Tolerance rate <span class="math notranslate nohighlight">\(\varepsilon\)</span>. The algorithm halts 
when the squared norm of the gradient of the loss value is smaller than <span class="math notranslate nohighlight">\(\varepsilon\)</span>.</p></li>
<li><p><strong>targets</strong> (np.ndarray): Target values at each time point <span class="math notranslate nohighlight">\(h\)</span>.</p></li>
<li><p><strong>crn_name</strong> (str): Name of the CRN to use for the files.</p></li>
<li><p><strong>weights</strong> (np.ndarray, optional): Weights of each target. Has shape <span class="math notranslate nohighlight">\((L,)\)</span>. If None, all targets
have the same weight. Defaults to None.</p></li>
<li><p><strong>directory</strong> (str, optional): Name of the directory under which to save the files. Must end with “/”.
If not “”, must end with “/”. Defaults to “”, which means no directory.</p></li>
<li><p><strong>save</strong> (Tuple[bool, list], optional): If the first argument is True, saves the file. The second argument is the name of the file under 
which to save the plot. Defaults to (True, [“control_values”, “experimental_losses”, “parameters”, “gradients_losses”, “real_losses”, “exp_results”]).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<p>The example file <code class="docutils literal notranslate"><span class="pre">training_pgd.py</span></code> runs the Projected Gradient Descent Algorithm using these functions.</p>
</section>
<section id="tuning-hyperparameters">
<h2>Tuning hyperparameters<a class="headerlink" href="#tuning-hyperparameters" title="Permalink to this heading"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="hyperparameters_test.test_comb">
<span class="sig-prename descclassname"><span class="pre">hyperparameters_test.</span></span><span class="sig-name descname"><span class="pre">test_comb</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">lr</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_rounds</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batchsize</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_hidden</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">valid_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_params</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_comps</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">early_stopping</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mixture</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'NB'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_models</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hyperparameters_test.html#test_comb"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#hyperparameters_test.test_comb" title="Permalink to this definition"></a></dt>
<dd><p>Trains a Mixture Density Network on the input data with the given combination of hyperparameters.</p>
<dl>
<dt>Args:</dt><dd><ul>
<li><p><span class="math notranslate nohighlight">\(l_r\)</span> (float): Initial learning rate.</p></li>
<li><p><strong>max_rounds</strong> (int): Maximal number of training rounds.</p></li>
<li><p><strong>batchsize</strong> (int): Number of elements in a batch.</p></li>
<li><p><strong>n_hidden</strong> (int): Number of neurons in the hidden layer.</p></li>
<li><p><strong>train_data</strong> (Tuple[torch.tensor]): Training dataset.</p></li>
<li><p><strong>valid_data</strong> (Tuple[torch.tensor]): Validation dataset.</p></li>
<li><p><strong>n_params</strong> (int): Number of CRN parameters in input, excluding time parameter <span class="math notranslate nohighlight">\(M_{\text{tot}}\)</span>.</p></li>
<li><p><strong>n_comps</strong> (int): Number of components of the output mixture.</p></li>
<li><p><strong>early_stopping</strong> (Tuple[bool, int, float]):</p>
<blockquote>
<div><ul class="simple">
<li><p>(bool): If True, uses the early stopping regularization. Defaults to False.</p></li>
<li><p><strong>patience</strong> (int): Patience level <span class="math notranslate nohighlight">\(n_p\)</span>.
At epoch <span class="math notranslate nohighlight">\(n\)</span>, the <span class="math notranslate nohighlight">\((n-n_p)\)</span> -th epoch is compared pairwise with that 
of the last <span class="math notranslate nohighlight">\(n_p\)</span> epochs. Defaults to <span class="math notranslate nohighlight">\(50\)</span>.</p></li>
<li><p><strong>delta</strong> (float): Tolerance threshold <span class="math notranslate nohighlight">\(\delta\)</span>. Training is stopped if the decrease between 
the elements of one of those pairs is lower than <span class="math notranslate nohighlight">\(\delta\)</span>. Defaults to <span class="math notranslate nohighlight">\(10^{-6}\)</span>.</p></li>
</ul>
</div></blockquote>
</li>
<li><p><strong>mixture</strong> (str, optional): Type of mixture to compute. Defaults to “NB” for a Negative Binomial Mixture.
Can also be “Poisson” for a Poisson mixture.</p></li>
<li><p><strong>n_models</strong> (int): Number of models to train. The return losses are the mean of the losses computed for each model.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>Losses for the training and validation datasets and a list of the hyperparameters used for the training.</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">list</span></code>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="hyperparameters_tuning.test_multiple_combs">
<span class="sig-prename descclassname"><span class="pre">hyperparameters_tuning.</span></span><span class="sig-name descname"><span class="pre">test_multiple_combs</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">testing_function</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lrs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_rounds</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batchsizes</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_hidden</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">early_stopping</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">file_name</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hyperparameters_tuning.html#test_multiple_combs"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#hyperparameters_tuning.test_multiple_combs" title="Permalink to this definition"></a></dt>
<dd><p>Tests all combinations of the given hyperparameters.</p>
<dl>
<dt>Args:</dt><dd><ul>
<li><p><strong>lrs</strong> (list): List of learning rates to test.</p></li>
<li><p><strong>max_rounds</strong> (list): List of maximal number of training rounds to test.</p></li>
<li><p><strong>batchsizes</strong> (list): List of batchsizes to test.</p></li>
<li><p><strong>n_hidden</strong> (list): List of number of hidden layer neurons to test.</p></li>
<li><p><strong>early_stopping</strong>:</p>
<blockquote>
<div><ul class="simple">
<li><p>(bool): If True, use the early stopping regularization. Defaults to False.</p></li>
<li><p><strong>patience</strong> (list): List of patience levels <span class="math notranslate nohighlight">\(n_p\)</span>.
At epoch <span class="math notranslate nohighlight">\(n\)</span>, the <span class="math notranslate nohighlight">\((n-n_p)\)</span> -th epoch is compared pairwise with that
of the last <span class="math notranslate nohighlight">\(n_p\)</span> epochs.</p></li>
<li><p><strong>delta</strong> (list): List of tolerance thresholds <span class="math notranslate nohighlight">\(\delta\)</span>. Training is stopped if the decrease between
the elements of one of those pairs is lower than <span class="math notranslate nohighlight">\(\delta\)</span>.</p></li>
</ul>
</div></blockquote>
</li>
<li><p><strong>file_name</strong> (str): Name of the CSV file under which to save the results. Each line of the CSV file corresponds to
one hyperparameter combination. The first elements of each line are the selected hyperparameters. The next and last 
2 elements of each line are the training and validation losses for these hyperparameters.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>Array of the training and validation losses for each combination of hyperparameters. Has shape 
<span class="math notranslate nohighlight">\((N_{\text{comb}}, 2)\)</span> where <span class="math notranslate nohighlight">\(N_{\text{comb}}\)</span> is the number of possible hyperparameters combinations.</p></li>
<li><p>Array of tested combinations of hyperparameters. Has shape <span class="math notranslate nohighlight">\((N_{\text{comb}}, M_{\text{tot}})\)</span>.</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code>]</p>
</dd>
</dl>
</dd></dl>

<p>The example file <code class="docutils literal notranslate"><span class="pre">tuning.py</span></code> tunes the hyperparameters for the Production and Degradation Chemical Reaction Network using these functions.</p>
</section>
<section id="comparing-with-the-finite-state-projection-method">
<h2>Comparing with the Finite State Projection method<a class="headerlink" href="#comparing-with-the-finite-state-projection-method" title="Permalink to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="fsp.StateSpaceEnumeration">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">fsp.</span></span><span class="sig-name descname"><span class="pre">StateSpaceEnumeration</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cr</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/fsp.html#StateSpaceEnumeration"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#fsp.StateSpaceEnumeration" title="Permalink to this definition"></a></dt>
<dd><p>State space enumeration as presented in <span id="id4">[<a class="reference internal" href="sources.html#id3" title="Ankit Gupta, Jan Mikelson, and Mustafa Khammash. A finite state projection algorithm for the stationary solution of the chemical master equation. The Journal of Chemical Physics., 147:154101, 2017.">GMK17</a>]</span>.
Computes functions <span class="math notranslate nohighlight">\(\Phi_{\text{dim}}\)</span> and <span class="math notranslate nohighlight">\(\Phi_{\text{dim}}^{-1}\)</span> to project the state space on the set of integers and conversely.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><span class="math notranslate nohighlight">\(C_r\)</span> (int): Value such that the projection of <span class="math notranslate nohighlight">\((0, .., 0, C_r)\)</span> is the last element of the projected truncated space.</p></li>
<li><p><strong>dim</strong> (int): Dimensions of the initial state-space.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="fsp.StateSpaceEnumeration.create_bijection">
<span class="sig-name descname"><span class="pre">create_bijection</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/fsp.html#StateSpaceEnumeration.create_bijection"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#fsp.StateSpaceEnumeration.create_bijection" title="Permalink to this definition"></a></dt>
<dd><p>Saves the bijection values in a dictionary.</p>
<p>A dictionary requires unhashable keys. For this reason, we use tuples instead of arrays.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="fsp.StateSpaceEnumeration.phi">
<span class="sig-name descname"><span class="pre">phi</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/fsp.html#StateSpaceEnumeration.phi"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#fsp.StateSpaceEnumeration.phi" title="Permalink to this definition"></a></dt>
<dd><p>Recurrent function which computes the projection <span class="math notranslate nohighlight">\(\Phi: \mathbb{N}^n \rightarrow \mathbb{N}\)</span> 
as defined in <span id="id5">[<a class="reference internal" href="sources.html#id3" title="Ankit Gupta, Jan Mikelson, and Mustafa Khammash. A finite state projection algorithm for the stationary solution of the chemical master equation. The Journal of Chemical Physics., 147:154101, 2017.">GMK17</a>]</span>.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>x</strong> (np.ndarray): Vector in <span class="math notranslate nohighlight">\(\mathbb{N}^n\)</span>.</p></li>
<li><p><strong>n</strong> (int): Dimension of the space.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>Scalar <span class="math notranslate nohighlight">\(\Phi_n(x)\)</span> in <span class="math notranslate nohighlight">\(\mathbb{N}\)</span>.</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="fsp.StateSpaceEnumeration.phi_inverse">
<span class="sig-name descname"><span class="pre">phi_inverse</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">z</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/fsp.html#StateSpaceEnumeration.phi_inverse"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#fsp.StateSpaceEnumeration.phi_inverse" title="Permalink to this definition"></a></dt>
<dd><p>Recurrent function which computes the inversed projection <span class="math notranslate nohighlight">\(\Phi^{-1}: \mathbb{N} \rightarrow \mathbb{N}^n\)</span> 
as defined in <span id="id6">[<a class="reference internal" href="sources.html#id3" title="Ankit Gupta, Jan Mikelson, and Mustafa Khammash. A finite state projection algorithm for the stationary solution of the chemical master equation. The Journal of Chemical Physics., 147:154101, 2017.">GMK17</a>]</span>.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>z</strong> (int): Input.</p></li>
<li><p><strong>n</strong> (int): Dimension of the space.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>Vector <span class="math notranslate nohighlight">\(\Phi_n^{-1}(z)\)</span> in <span class="math notranslate nohighlight">\(\mathbb{N}^n\)</span>.</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="fsp.SensitivitiesDerivation">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">fsp.</span></span><span class="sig-name descname"><span class="pre">SensitivitiesDerivation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">crn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_time_windows</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">50</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/fsp.html#SensitivitiesDerivation"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#fsp.SensitivitiesDerivation" title="Permalink to this definition"></a></dt>
<dd><p>Class to compute the sensitivity of the likelihood and the probability mass function with the FSP method.
Based on <span id="id7">[<a class="reference internal" href="sources.html#id2" title="Zachary R Fox and Brian Munsky. The finite state projection based fisher information matrix approach to estimate information and optimize single-cell experiments. PLoS Computational Biology., 15:e1006365, 2019.">FM19</a>]</span>.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>crn</strong> (simulation.CRN): The CRN to work on.</p></li>
<li><p><strong>n_time_windows</strong> (int): Number of time windows <span class="math notranslate nohighlight">\(L\)</span>.</p></li>
<li><p><strong>index</strong> (Tuple[list, int], optional): Index of the parameters of interest. Can either be a
single integer value or a list of integer values. If it is a control parameter, considers the corresponding 
parameters for each time window, ie for a given index <span class="math notranslate nohighlight">\(i\)</span> of a control parameter <span class="math notranslate nohighlight">\(\xi^i\)</span>, 
the list <cite>index</cite> will include <span class="math notranslate nohighlight">\([\xi_1^i, ..., \xi_L^i]\)</span>. When None, computes the sensitivity of the likelihood for each parameter. 
The values of <cite>index</cite> thus are in <span class="math notranslate nohighlight">\([\![1, M_{\theta}+M_{\xi}]\!]\)</span>. Defaults to None.</p></li>
<li><p><span class="math notranslate nohighlight">\(C_r\)</span> (int, optional): Value such that <span class="math notranslate nohighlight">\((0, .., 0, C_r)\)</span> is the last value in the truncated space. 
Defaults to <span class="math notranslate nohighlight">\(50\)</span>.</p></li>
</ul>
</dd>
</dl>
<p>To simplify the notations in the following, we will omit the writing of parameters <span class="math notranslate nohighlight">\(\xi\)</span>. This amounts to
considering <span class="math notranslate nohighlight">\([\theta_1, ..., \theta_{M_{\theta}}, \xi_1^1, \xi_1^2, ..., \xi_1^{M_{\xi}}, ..., \xi_L^{M_{\xi}}] = [\theta_1, ..., \theta_{M'}]\)</span>.</p>
<dl class="py method">
<dt class="sig sig-object py" id="fsp.SensitivitiesDerivation.constant_matrix">
<span class="sig-name descname"><span class="pre">constant_matrix</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/fsp.html#SensitivitiesDerivation.constant_matrix"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#fsp.SensitivitiesDerivation.constant_matrix" title="Permalink to this definition"></a></dt>
<dd><p>Computes the constant matrix <span class="math notranslate nohighlight">\(\hat{C}^\theta\)</span> to solve the set of ODEs, possibly for several parameters at once.</p>
<p>In the following, we can easily assume that there are no controlled reaction. 
The expression can easily be generalised to the case where there are control parameters <span class="math notranslate nohighlight">\(\xi\)</span>.</p>
<p>Let us introduce <span class="math notranslate nohighlight">\(\alpha \in [\![0, M_{\theta}]\!]\)</span>, and let <span class="math notranslate nohighlight">\(I=\{i_1, i_2, ..., i_{\alpha}\} \in [\![1, M_{\theta}]\!]^\alpha\)</span>
be the set of parameters indices defined in <cite>index</cite>. The constant matrix <span class="math notranslate nohighlight">\(\hat{C}^\theta\)</span> in the set of ODEs to solve the sensitivities 
with respect to several parameters is given by:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\hat{C}^\theta = \begin{pmatrix} \hat{A}^\theta &amp; 0 &amp; 0 &amp; ... &amp; 0 \\ \frac{\partial \hat{A}^\theta}{\partial \theta_{i_1}} &amp; \hat{A}^\theta &amp; 0 &amp; ... &amp; 0
\\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 
\frac{\partial \hat{A}^\theta}{\partial \theta_{i_\alpha}} &amp; 0 &amp; 0 &amp; ... &amp; \hat{A}^\theta \end{pmatrix}\end{split}\]</div>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>params</strong> (np.ndarray): Current parameters of the propensity functions. Has shape <span class="math notranslate nohighlight">\((M_{\theta} + M_{\xi},)\)</span>.</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="fsp.SensitivitiesDerivation.create_gdrv">
<span class="sig-name descname"><span class="pre">create_gdrv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">params</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ind</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/fsp.html#SensitivitiesDerivation.create_gdrv"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#fsp.SensitivitiesDerivation.create_gdrv" title="Permalink to this definition"></a></dt>
<dd><p>Computes <span class="math notranslate nohighlight">\(\frac{\partial \hat{A}^\theta}{\partial \theta_{\text{ind}}}\)</span> in the
case of non-mass-action kinetics.</p>
<p>Requires the propensity derivatives to be explicitly defined.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>params</strong> (np.ndarray): Current parameters of the propensity functions.</p></li>
<li><p><strong>index</strong> (int): Index of the parameter from which <span class="math notranslate nohighlight">\(\hat{A}^\theta\)</span> is derived.</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="fsp.SensitivitiesDerivation.create_gdrv_B">
<span class="sig-name descname"><span class="pre">create_gdrv_B</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ind</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/fsp.html#SensitivitiesDerivation.create_gdrv_B"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#fsp.SensitivitiesDerivation.create_gdrv_B" title="Permalink to this definition"></a></dt>
<dd><p>Computes <span class="math notranslate nohighlight">\(\frac{\partial \hat{A}^\theta}{\partial \theta_{\text{ind}}}\)</span> in the case of mass-action kinetics.
In that case, <span class="math notranslate nohighlight">\(\frac{\partial\hat{A}^\theta}{\partial \theta_{\text{ind}}} = \hat{B}_{\text{ind}}\)</span> 
where the rate matrix <span class="math notranslate nohighlight">\(\hat{B}_i\)</span> is as defined in <span id="id8">[<a class="reference internal" href="sources.html#id2" title="Zachary R Fox and Brian Munsky. The finite state projection based fisher information matrix approach to estimate information and optimize single-cell experiments. PLoS Computational Biology., 15:e1006365, 2019.">FM19</a>]</span>.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>ind</strong> (int): Index of the parameter from which <span class="math notranslate nohighlight">\(\hat{A}^\theta\)</span> is derived.</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="fsp.SensitivitiesDerivation.create_generator">
<span class="sig-name descname"><span class="pre">create_generator</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/fsp.html#SensitivitiesDerivation.create_generator"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#fsp.SensitivitiesDerivation.create_generator" title="Permalink to this definition"></a></dt>
<dd><p>Computes the generator matrix <span class="math notranslate nohighlight">\(\hat{A}^\theta\)</span> as defined in <span id="id9">[<a class="reference internal" href="sources.html#id2" title="Zachary R Fox and Brian Munsky. The finite state projection based fisher information matrix approach to estimate information and optimize single-cell experiments. PLoS Computational Biology., 15:e1006365, 2019.">FM19</a>]</span></p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>params</strong> (np.ndarray): Current parameters of the propensity functions.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>Generator <span class="math notranslate nohighlight">\(\hat{A}^\theta\)</span> in the general case of non-mass-action kinetics.</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="fsp.SensitivitiesDerivation.create_generator_derivative">
<span class="sig-name descname"><span class="pre">create_generator_derivative</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">params</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ind</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/fsp.html#SensitivitiesDerivation.create_generator_derivative"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#fsp.SensitivitiesDerivation.create_generator_derivative" title="Permalink to this definition"></a></dt>
<dd><p>Computes <span class="math notranslate nohighlight">\(\frac{\partial \hat{A}^\theta}{\partial \theta_{\text{ind}}}\)</span> in the general case.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>params</strong> (np.ndarray): Current parameters of the propensity functions.</p></li>
<li><p><strong>ind</strong> (int): Index of the parameter from which <span class="math notranslate nohighlight">\(\hat{A}^\theta\)</span> is derived.</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="fsp.SensitivitiesDerivation.expected_val">
<span class="sig-name descname"><span class="pre">expected_val</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sampling_times</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_windows</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parameters</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ind_species</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/fsp.html#SensitivitiesDerivation.expected_val"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#fsp.SensitivitiesDerivation.expected_val" title="Permalink to this definition"></a></dt>
<dd><p>Computes the expected value at each sampling time <span class="math notranslate nohighlight">\(E_{\theta, \xi}[X_t]\)</span>.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>sampling_times</strong> (np.ndarray): Sampling times.</p></li>
<li><p><strong>time_windows</strong> (np.ndarray): Time windows during which the parameters do not vary. Its form is <span class="math notranslate nohighlight">\([t_1, ..., t_L]\)</span>,
such that the considered time windows are <span class="math notranslate nohighlight">\([0, t_1], [t_1, t_2], ..., [t_{L-1}, t_L]\)</span>. <span class="math notranslate nohighlight">\(t_L\)</span> must match
with the final time <span class="math notranslate nohighlight">\(t_f\)</span>. If there is only one time window, <strong>time_windows</strong> should be defined as <span class="math notranslate nohighlight">\([t_f]\)</span>.</p></li>
<li><p><strong>parameters</strong> (np.ndarray): Parameters of the propensity functions.</p></li>
<li><p><strong>ind_species</strong> (int): Index of the species of interest.</p></li>
</ul>
</dd>
</dl>
<p>Output has shape <span class="math notranslate nohighlight">\((L,)\)</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="fsp.SensitivitiesDerivation.gradient_expected_val">
<span class="sig-name descname"><span class="pre">gradient_expected_val</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sampling_times</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_windows</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parameters</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ind_species</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">with_probs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/fsp.html#SensitivitiesDerivation.gradient_expected_val"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#fsp.SensitivitiesDerivation.gradient_expected_val" title="Permalink to this definition"></a></dt>
<dd><p>Computes the gradient of the expected value with respect to one or several parameters defined in <cite>index</cite>.</p>
<div class="math notranslate nohighlight">
\[\nabla_{\theta, \xi} E_{\theta, \xi}[X_t] = \sum_{k \in \mathbb{N}} k \ \nabla_{\theta, \xi} p(k;t,\theta,\xi)\]</div>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>sampling_times</strong> (np.ndarray): Sampling times.</p></li>
<li><p><strong>time_windows</strong> (np.ndarray): Time windows during which the parameters do not vary. Its form is <span class="math notranslate nohighlight">\([t_1, ..., t_L]\)</span>,
such that the considered time windows are <span class="math notranslate nohighlight">\([0, t_1], [t_1, t_2], ..., [t_{L-1}, t_L]\)</span>. <span class="math notranslate nohighlight">\(t_L\)</span> must match
with the final time <span class="math notranslate nohighlight">\(t_f\)</span>. If there is only one time window, <strong>time_windows</strong> should be defined as <span class="math notranslate nohighlight">\([t_f]\)</span>.</p></li>
<li><p><strong>parameters</strong> (np.ndarray): Parameters of the propensity functions.</p></li>
<li><p><strong>ind_species</strong> (int): Index of the species of interest.</p></li>
<li><p><strong>with_probs</strong> (bool): If True, returns the gradient of the expectation and the expectation.
If False, only returns the gradient of the expected value.</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code>]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="fsp.SensitivitiesDerivation.marginal">
<span class="sig-name descname"><span class="pre">marginal</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sampling_times</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_windows</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parameters</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ind_species</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">with_stv</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/fsp.html#SensitivitiesDerivation.marginal"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#fsp.SensitivitiesDerivation.marginal" title="Permalink to this definition"></a></dt>
<dd><p>Computes the marginal probability mass functions and marginal sensitivities of the likelihood.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>sampling_times</strong> (np.ndarray): Sampling times.</p></li>
<li><p><strong>time_windows</strong> (np.ndarray): Time windows during which the parameters do not vary. Its form is <span class="math notranslate nohighlight">\([t_1, ..., t_L]\)</span>,
such that the considered time windows are <span class="math notranslate nohighlight">\([0, t_1], [t_1, t_2], ..., [t_{L-1}, t_L]\)</span>. <span class="math notranslate nohighlight">\(t_L\)</span> must match
with the final time <span class="math notranslate nohighlight">\(t_f\)</span>. If there is only one time window, <strong>time_windows</strong> should be defined as <span class="math notranslate nohighlight">\([t_f]\)</span>.</p></li>
<li><p><strong>parameters</strong> (np.ndarray): Parameters of the propensity functions.</p></li>
<li><p><strong>ind_species</strong> (int): Index of the species of interest.</p></li>
<li><p><strong>with_stv</strong> (bool, optional): If True, computes the sensitivity of the likelihood. 
If False, computes only the probability distribution. Defaults to True.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>The marginal probability and, if <strong>with_stv</strong> is True, the marginal sensitivities of the likelihood for each sampling time.
Has shape <span class="math notranslate nohighlight">\((N_{\max}, L, \text{len}(\text{index})+1)\)</span> if <strong>with_stv</strong> is True and <span class="math notranslate nohighlight">\((N_{\max}, L, 1)\)</span> otherwise.</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="fsp.SensitivitiesDerivation.marginals">
<span class="sig-name descname"><span class="pre">marginals</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sampling_times</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_windows</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parameters</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ind_species</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">with_stv</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/fsp.html#SensitivitiesDerivation.marginals"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#fsp.SensitivitiesDerivation.marginals" title="Permalink to this definition"></a></dt>
<dd><p>Computes the marginal probability mass functions and marginal sensitivity of the likelihood for multiple species.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>sampling_times</strong> (np.ndarray): Sampling times.</p></li>
<li><p><strong>time_windows</strong> (np.ndarray): Time windows during which the parameters do not vary. Its form is <span class="math notranslate nohighlight">\([t_1, ..., t_L]\)</span>,
such that the considered time windows are <span class="math notranslate nohighlight">\([0, t_1], [t_1, t_2], ..., [t_{L-1}, t_L]\)</span>. <span class="math notranslate nohighlight">\(t_L\)</span> must match
with the final time <span class="math notranslate nohighlight">\(t_f\)</span>. If there is only one time window, <strong>time_windows</strong> should be defined as <span class="math notranslate nohighlight">\([t_f]\)</span>.</p></li>
<li><p><strong>parameters</strong> (np.ndarray): Parameters of the propensity functions.</p></li>
<li><p><strong>ind_species</strong> (list): List of index of the species of interest.</p></li>
<li><p><strong>with_stv</strong> (bool, optional): If True, computes the sensitivities of the likelihood. 
If False, only computes the probability distribution. Defaults to True.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>Each key of the dictionary is the index of one species. Its value is the marginal distribution as returned by
the function <code class="docutils literal notranslate"><span class="pre">marginal</span></code> for this species.</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="fsp.SensitivitiesDerivation.reset">
<span class="sig-name descname"><span class="pre">reset</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/fsp.html#SensitivitiesDerivation.reset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#fsp.SensitivitiesDerivation.reset" title="Permalink to this definition"></a></dt>
<dd><p>Resets the class to the initial setting: sets the time to <span class="math notranslate nohighlight">\(t=0\)</span>, the current time window to <span class="math notranslate nohighlight">\(0\)</span>
and the current state to the initial state.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="fsp.SensitivitiesDerivation.solve_multiple_odes">
<span class="sig-name descname"><span class="pre">solve_multiple_odes</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sampling_times</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_windows</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parameters</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">with_stv</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/fsp.html#SensitivitiesDerivation.solve_multiple_odes"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#fsp.SensitivitiesDerivation.solve_multiple_odes" title="Permalink to this definition"></a></dt>
<dd><p>Solves the set of ODEs for several parameters at once, taking into account varying parameters over time windows.
This means that, in case there is a control reaction:</p>
<div class="math notranslate nohighlight">
\[\forall i \in [\![1, M_{\xi}]\!], \forall j \in [\![1, L]\!], \frac{\partial}{\partial t}\frac{\partial \hat{p}^{\theta, \xi}}{\partial \xi_j^i}(t)
= 0 \text{ if } t &lt; t_{j-1}\]</div>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>sampling_times</strong> (np.ndarray): Sampling times.</p></li>
<li><p><strong>time_windows</strong> (np.ndarray): Time windows during which the parameters do not vary.
Its form is <span class="math notranslate nohighlight">\([t_1, ..., t_L]\)</span>, such that the considered time windows are 
<span class="math notranslate nohighlight">\([0, t_1], [t_1, t_2], ..., [t_{L-1}, t_L]\)</span>. <span class="math notranslate nohighlight">\(t_L\)</span> must match with the final time 
<span class="math notranslate nohighlight">\(t_f\)</span>. If there is only one time window, <strong>time_windows</strong> should be defined as <span class="math notranslate nohighlight">\([t_f]\)</span>.</p></li>
<li><p><strong>parameters</strong> (np.ndarray): Parameters of the propensity functions for each time window.
Has shape <span class="math notranslate nohighlight">\((L, M_{\theta}+M_{\xi})\)</span>.</p></li>
<li><p><strong>with_stv</strong> (bool, optional): If True, computes the sensitivities of the likelihood 
with respect to the indices in <cite>self.index</cite>. If False, computes only the probability distribution. 
Defaults to True.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>The probability and, if <strong>with_stv</strong> is True, the sensitivity distributions for each sampling time.
Has shape <span class="math notranslate nohighlight">\((N_{\max}, L, \text{len}(\text{index})+1)\)</span> if <strong>with_stv</strong> is True and <span class="math notranslate nohighlight">\((N_{\max}, L, 1)\)</span>
if <strong>with_stv</strong> is False.</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">ndarray</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="fsp.SensitivitiesDerivation.solve_ode">
<span class="sig-name descname"><span class="pre">solve_ode</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">init_state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">t0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tf</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">params</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">t_eval</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">with_stv</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/fsp.html#SensitivitiesDerivation.solve_ode"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#fsp.SensitivitiesDerivation.solve_ode" title="Permalink to this definition"></a></dt>
<dd><p>Solves the following set of linear ODEs, which allows to solve the sensitivities with respect to several parameters at once:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial}{\partial t}\begin{pmatrix} \hat{p}^{\theta, \xi} \\ \hat{S}_{i_1}^{\theta, \xi} \\ \vdots \\ \hat{S}^{\theta,\xi}_{i_\alpha} \end{pmatrix} 
= \hat{C}^{\theta,\xi}
\begin{pmatrix} \hat{p}^{\theta,\xi} \\ \hat{S}^{\theta,\xi}_{i_1} \\ \vdots \\ \hat{S}^{\theta,\xi}_{i_\alpha} \end{pmatrix}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(I=\{i_1, i_2, ..., i_{\alpha}\} \subset [\![1, M_{\theta}+M_{\xi}]\!]\)</span> is the set of parameters indices defined in <cite>index</cite>
and <span class="math notranslate nohighlight">\(\hat{C}^{\theta,\xi}\)</span> is as defined in the function <code class="docutils literal notranslate"><span class="pre">constant_matrix</span></code>.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>init_state</strong> (np.ndarray): Initial state for probabilities and sensitivity of the likelihood.
Has shape <span class="math notranslate nohighlight">\((N_{\max}\times(M_{\text{tot}}+1),)\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(t_0\)</span> (float): Starting time.</p></li>
<li><p><span class="math notranslate nohighlight">\(t_f\)</span> (float): Final time.</p></li>
<li><p><strong>params</strong> (np.ndarray): Parameters of the propensity functions. Has shape <span class="math notranslate nohighlight">\((M_{\theta}+M_{\xi},)\)</span>.</p></li>
<li><p><strong>t_eval</strong> (list): Sampling times.</p></li>
<li><p><strong>with_stv</strong> (bool): If True, computes the corresponding sensitivities. If False, only solves the first part 
of the ODE to compute the probability mass function. Defaults to True.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>Bunch object as the output of the <code class="docutils literal notranslate"><span class="pre">solve_ivp</span></code> function applied to the set of linear ODEs.
To access the probability and sensitivities distributions, use the key ‘y’. The result has shape 
<span class="math notranslate nohighlight">\((N_{\max}, \text{n_sampling_times})\)</span> if <strong>with_stv</strong> is False, <span class="math notranslate nohighlight">\((2 \times N_{\max}, \text{n_sampling_times})\)</span> otherwise.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="projected_gradient_descent.ProjectedGradientDescent_FSP">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">projected_gradient_descent.</span></span><span class="sig-name descname"><span class="pre">ProjectedGradientDescent_FSP</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">crn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ind_species</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">domain</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fixed_params</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_windows</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_loss</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">50</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/projected_gradient_descent.html#ProjectedGradientDescent_FSP"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#projected_gradient_descent.ProjectedGradientDescent_FSP" title="Permalink to this definition"></a></dt>
<dd><p>Class to compute the Projected Gradient Descent based on the Finite State Projection method.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>crn</strong> (simulation.CRN): CRN to work on.</p></li>
<li><p><strong>ind_species</strong> (int): Index of the species to study.</p></li>
<li><p><strong>domain</strong> (np.ndarray): Boundaries of the domain in which to project. Has shape <span class="math notranslate nohighlight">\((\text{dim}, 2)\)</span>.
<strong>domain[:,0]</strong> defines the lower boundaries for each dimension, <strong>domain[:,1]</strong> defines the 
upper boundaries for each dimension.</p></li>
<li><p><strong>fixed_params</strong> (np.ndarray): Selected values for the fixed parameters.</p></li>
<li><p><strong>time_windows</strong> (np.ndarray): Time windows during which the parameters do not vary.
Its form is <span class="math notranslate nohighlight">\([t_1, ..., t_L]\)</span>, such that the time windows are 
<span class="math notranslate nohighlight">\([0, t_1], [t_1, t_2], ..., [t_{L-1}, t_L]\)</span>. <span class="math notranslate nohighlight">\(t_L\)</span> must match with the final time 
<span class="math notranslate nohighlight">\(t_f\)</span>. If there is only one time window, <strong>time_windows</strong> should be defined as <span class="math notranslate nohighlight">\([t_f]\)</span>.</p></li>
<li><p><strong>loss</strong> (Union[Callable, list]): Loss function used for the gradient descent. If it is a list, each element
is the loss function for the corresponding time window.</p></li>
<li><p><strong>grad_loss</strong> (Union[Callable, list]): Gradient of the loss function. If it is a list, each element is the gradient
of the loss function for the corresponding time window.</p></li>
<li><p><strong>weights</strong> (np.ndarray, optional):  If True, sensitivities are set to zero when control parameters
have no influence on that time window. If False, works with the computed sensitivities. Defaults to True.</p></li>
<li><p><span class="math notranslate nohighlight">\(C_r\)</span> (int, optional): Value such that <span class="math notranslate nohighlight">\((0, .., 0, C_r)\)</span> is the last value in the truncated space. 
Defaults to <span class="math notranslate nohighlight">\(50\)</span>.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="projected_gradient_descent.ProjectedGradientDescent_FSP.create_gradient">
<span class="sig-name descname"><span class="pre">create_gradient</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/projected_gradient_descent.html#ProjectedGradientDescent_FSP.create_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#projected_gradient_descent.ProjectedGradientDescent_FSP.create_gradient" title="Permalink to this definition"></a></dt>
<dd><p>Computes the gradient function of the loss evaluated at the expected value with respect to 
all control parameters.</p>
<div class="math notranslate nohighlight">
\[\xi \mapsto \nabla_{\xi} L\big(E_{\theta, \xi}[X_t]\big) = \frac{dL(x)}{dx} \nabla_{\xi} E_{\theta, \xi}[X_t]\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="projected_gradient_descent.ProjectedGradientDescent_FSP.create_loss">
<span class="sig-name descname"><span class="pre">create_loss</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/projected_gradient_descent.html#ProjectedGradientDescent_FSP.create_loss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#projected_gradient_descent.ProjectedGradientDescent_FSP.create_loss" title="Permalink to this definition"></a></dt>
<dd><p>Computes the loss function evaluated at the expectation.</p>
<div class="math notranslate nohighlight">
\[\xi \mapsto \mathcal{L}\big( E_{\theta, \xi}[X_t]\big)\]</div>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="training_pgd.pgdFSP">
<span class="sig-prename descclassname"><span class="pre">training_pgd.</span></span><span class="sig-name descname"><span class="pre">pgdFSP</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">crn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ind_species</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">domain</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fixed_params</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_windows</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_loss</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cr</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_iter</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_loss</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">targets</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">crn_name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">directory</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(True,</span> <span class="pre">['control_values',</span> <span class="pre">'experimental_losses',</span> <span class="pre">'parameters',</span> <span class="pre">'gradients_losses',</span> <span class="pre">'real_losses',</span> <span class="pre">'exp_results'])</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/training_pgd.html#pgdFSP"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#training_pgd.pgdFSP" title="Permalink to this definition"></a></dt>
<dd><p>Performs the PGD with the FSP method, saves the selected parameters for the algorithm and the results in a <code class="docutils literal notranslate"><span class="pre">.txt</span></code> file,
plots the results and saves them in CSV files.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>crn</strong> (simulation.CRN): CRN to work on. Make sure to specify the derivatives of the propensities if the CRN does not follow
mass-action kinetics.</p></li>
<li><p><strong>ind_species</strong> (int): Index of the species of interest.</p></li>
<li><p><strong>domain</strong> (np.ndarray): Boundaries of the domain in which to project. Has shape <span class="math notranslate nohighlight">\((\text{dim}, 2)\)</span>.
<strong>domain[:,0]</strong> defines the lower boundaries for each dimension, <strong>domain[:,1]</strong> defines the 
upper boundaries for each dimension.</p></li>
<li><p><strong>fixed_params</strong> (np.ndarray): Selected values for the fixed parameters <span class="math notranslate nohighlight">\(\theta\)</span>.</p></li>
<li><p><strong>time_windows</strong> (np.ndarray): Time windows during which all parameters are fixed. 
Its form is <span class="math notranslate nohighlight">\([t_1, ..., t_L]\)</span>, such that the time windows are 
<span class="math notranslate nohighlight">\([0, t_1], [t_1, t_2], ..., [t_{L-1}, t_L]\)</span>. <span class="math notranslate nohighlight">\(t_L\)</span> must match with the final time 
<span class="math notranslate nohighlight">\(t_f\)</span>. If there is only one time window, <strong>time_windows</strong> should be defined as <span class="math notranslate nohighlight">\([t_f]\)</span>.</p></li>
<li><p><strong>loss</strong> (Union[Callable, list]): Loss function used for the gradient descent. If it is a list, each element
is the loss function for the corresponding time window.</p></li>
<li><p><strong>grad_loss</strong> (Union[Callable, list]): Gradient of the loss function. If it is a list, each element is the gradient
of the loss function for the corresponding time window.</p></li>
<li><p><span class="math notranslate nohighlight">\(C_r\)</span> (int): Value such that <span class="math notranslate nohighlight">\((0, .., 0, C_r)\)</span> is the last value in the truncated space. 
Defaults to <span class="math notranslate nohighlight">\(50\)</span>.</p></li>
<li><p><strong>gamma</strong> (float): Step size <span class="math notranslate nohighlight">\(\gamma\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(n_{\text{iter}}\)</span> (int): Maximal number of iterations allowed for the gradient descent.</p></li>
<li><p><strong>eps</strong> (float): Tolerance rate <span class="math notranslate nohighlight">\(\varepsilon\)</span>. The algorithm halts when the squared norm of the gradient of the loss value is smaller than <span class="math notranslate nohighlight">\(\varepsilon\)</span>.</p></li>
<li><p><strong>min_loss</strong> (float, optional): Minimal loss value. The algorithm halts when the loss value is smaller than <strong>min_loss</strong>.</p></li>
<li><p><strong>targets</strong> (np.ndarray): Target values at each time point <span class="math notranslate nohighlight">\(h\)</span>.</p></li>
<li><p><strong>crn_name</strong> (str): Name of the CRN to use for the files.</p></li>
<li><p><strong>weights</strong> (np.ndarray, optional): Weights of each target. Has shape <span class="math notranslate nohighlight">\((L,)\)</span>. If None, all targets
have the same weight. Defaults to None.</p></li>
<li><p><strong>directory</strong> (str, optional): Name of the directory under which to save the files. Must end with “/”. Defaults to “”, which means no directory.</p></li>
<li><p><strong>save</strong> (Tuple[bool, list], optional): If the first argument is True, saves the file. The second argument is the name of the file under 
which to save the plot. Defaults to (True, [“control_values”, “experimental_losses”, “parameters”, “gradients_losses”, “real_losses”, “exp_results”]).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="plotting-distributions-fisher-information-and-expectations">
<h2>Plotting distributions, Fisher Information and expectations<a class="headerlink" href="#plotting-distributions-fisher-information-and-expectations" title="Permalink to this heading"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="plot.plot_model">
<span class="sig-prename descclassname"><span class="pre">plot.</span></span><span class="sig-name descname"><span class="pre">plot_model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">to_pred</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">models</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">up_bound</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_windows</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_comps</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index_names</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">('Probabilities',</span> <span class="pre">'Abundance</span> <span class="pre">of</span> <span class="pre">species</span> <span class="pre">$S$')</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">plot_test_result</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(False,</span> <span class="pre">None)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">plot_exact_result</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(False,</span> <span class="pre">None)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">plot_fsp_result</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(False,</span> <span class="pre">None)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">plot</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">('probabilities',</span> <span class="pre">None)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(False,</span> <span class="pre">None)</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/plot.html#plot_model"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#plot.plot_model" title="Permalink to this definition"></a></dt>
<dd><p>Plots distributions estimated with various methods for a single set of time and parameters and for a specified CRN.</p>
<dl>
<dt>Args:</dt><dd><ul>
<li><p><strong>to_pred</strong> (torch.tensor): Time and parameters in the form requested by the Mixture Density Networks: 
<span class="math notranslate nohighlight">\([t, \theta_1, ..., \theta_{M_{\theta}}, \xi_1^1, \xi_1^2, ..., \xi_1^{M_{\xi}}, \xi_2^1, ..., \xi_L^{M_{\xi}}]\)</span>.</p></li>
<li><p><strong>models</strong> (list): Mixture Density Network models to use.</p></li>
<li><p><strong>up_bound</strong> (int): Upper boundary of the predicted distribution.</p></li>
<li><p><strong>time_windows</strong> (np.ndarray): Time windows during which the parameters do not vary. Its form is <span class="math notranslate nohighlight">\([t_1, ..., t_L]\)</span>,
such that the considered time windows are <span class="math notranslate nohighlight">\([0, t_1], [t_1, t_2], ..., [t_{L-1}, t_L]\)</span>. <span class="math notranslate nohighlight">\(t_L\)</span> must match
with the final time <span class="math notranslate nohighlight">\(t_f\)</span>. If there is only one time window, <strong>time_windows</strong> should be defined as <span class="math notranslate nohighlight">\([t_f]\)</span>.</p></li>
<li><p><strong>n_comps</strong> (int): Number of components of the predicted mixture.</p></li>
<li><p><strong>index_names</strong> (Tuple[str, str], optional): Labels of x-axis and y-axis. Defaults to (“Probabilities”, “Abundance of species S”).</p></li>
<li><p><strong>plot_test_result</strong> (Tuple[bool, torch.tensor], optional): If the first argument is True, plots the expected results 
from the datasets for the chosen set of parameters. The second argument is the expected results. Defaults to (False, None).</p></li>
<li><p><strong>plot_exact_result</strong> (Tuple[bool, Callable], optional): If the first argument is True, plots the exact results 
for the chosen set of parameters. The second argument is the function that computes the exact results. Defaults to (False, None).</p></li>
<li><p><strong>plot_fsp_result</strong> (Tuple[bool, np.ndarray, np.ndarray, int, np.ndarray], optional): If the first argument is True, 
plots the estimated results with the FSP method.</p>
<blockquote>
<div><ol class="arabic simple">
<li><p><strong>fsp_estimation</strong> (bool): If True, estimates the distribution with the FSP method. Defaults to False.</p></li>
<li><p><strong>stoich_mat</strong> (np.ndarray): Stoichiometry matrix.</p></li>
<li><p><strong>propensities</strong> (np.ndarray): Non-parameterised propensity functions.</p></li>
<li><p><strong>propensities_drv</strong> (np.ndarray): Gradient functions of the propensities with respect to the parameters.
Has shape <span class="math notranslate nohighlight">\((M, M_{\theta}+M_{\xi})\)</span>. If None, it is assumed that the Chemical Reaction Network follows mass-action kinetics.</p></li>
<li><p><span class="math notranslate nohighlight">\(C_r\)</span>: Integer such that the projection of <span class="math notranslate nohighlight">\((0, .., 0, C_r)\)</span> is the last element of the projected truncated space.</p></li>
<li><p><strong>init_state</strong> (Tuple[int], optional): Initial state. If None, the initial state is set to <span class="math notranslate nohighlight">\((0,..,0)\)</span>.</p></li>
<li><p><strong>ind_species</strong> (int): Index of the species of interest.</p></li>
<li><p><strong>n_fixed_params</strong> (int): Number of fixed parameters required to define the propensity functions <span class="math notranslate nohighlight">\(M_{\theta}\)</span>.</p></li>
<li><p><strong>n_control_params</strong> (int): Number of control parameters required to define the propensity functions <span class="math notranslate nohighlight">\(M_{\xi}\)</span>.
Their values vary from one time window to another.</p></li>
</ol>
</div></blockquote>
</li>
<li><p><strong>plot</strong> (Tuple[str, int], optional): The first argument is either “probabilities” to plot a probability distribution, or “sensitivities”
to plot a sensitivity of the likelihood distribution. If it is “sensitivities”, the second argument is the index of the parameter 
such that it plots the sensitivities with respect to this parameter. Defaults to (“probabilities”, None).</p></li>
<li><p><strong>save</strong> (Tuple[bool, str], optional): If the first argument is True, saves the file. The second argument is the name of the file 
under which to save the plot. Defaults to (False, None).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="plot.multiple_plots">
<span class="sig-prename descclassname"><span class="pre">plot.</span></span><span class="sig-name descname"><span class="pre">multiple_plots</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">to_pred</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">models</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">up_bound</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_windows</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_comps</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index_names</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">('Probabilities',</span> <span class="pre">'Abundance</span> <span class="pre">of</span> <span class="pre">species</span> <span class="pre">$S$')</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">plot_test_result</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(False,</span> <span class="pre">None)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">plot_exact_result</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(False,</span> <span class="pre">None)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">plot_fsp_result</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(False,</span> <span class="pre">None)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">plot</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">('probabilities',</span> <span class="pre">None)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_col</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(False,</span> <span class="pre">None)</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/plot.html#multiple_plots"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#plot.multiple_plots" title="Permalink to this definition"></a></dt>
<dd><p>Plots distributions estimated with various methods for multiple sets of time and parameters and for a specified CRN.</p>
<dl>
<dt>Args:</dt><dd><ul>
<li><p><strong>to_pred</strong> (list): Time and parameters in the form requested by the Mixture Density Networks:
<span class="math notranslate nohighlight">\([t, \theta_1, ..., \theta_{M_{\theta}}, \xi_1^1, \xi_1^2, ..., \xi_1^{M_{\xi}}, \xi_2^1, ..., \xi_L^{M_{\xi}}]\)</span>.</p></li>
<li><p><strong>models</strong> (list): Mixture Density Network models to use.</p></li>
<li><p><strong>up_bound</strong> (int): Upper boundary of the predicted distributions.</p></li>
<li><p><strong>time_windows</strong> (np.ndarray): Time windows during which the parameters do not vary. Its form is <span class="math notranslate nohighlight">\([t_1, ..., t_L]\)</span>,
such that the considered time windows are <span class="math notranslate nohighlight">\([0, t_1], [t_1, t_2], ..., [t_{L-1}, t_L]\)</span>. <span class="math notranslate nohighlight">\(t_L\)</span> must match
with the final time <span class="math notranslate nohighlight">\(t_f\)</span>. If there is only one time window, <strong>time_windows</strong> should be defined as <span class="math notranslate nohighlight">\([t_f]\)</span>.</p></li>
<li><p><strong>n_comps</strong> (int): Number of components of the predicted mixture.</p></li>
<li><p><strong>index_names</strong> (Tuple[str], optional): Labels of x-axis and y-axis. Defaults to (“Probabilities”, “Abundance of species S”).</p></li>
<li><p><strong>plot_test_result</strong> (Tuple[bool, torch.tensor], optional): If the first argument is True, plots the expected results 
from the datasets for the chosen set of parameters. The second argument is the expected results. Defaults to (False, None).</p></li>
<li><p><strong>plot_exact_result</strong> (Tuple[bool, Callable], optional): If the first argument is True, plots the exact results for the
chosen set of parameters. The second argument is the function that computes the exact results. Defaults to (False, None).</p></li>
<li><p><strong>plot_fsp_result</strong> (Tuple[bool, np.ndarray, np.ndarray, int, np.ndarray, int, int, int], optional): If the first argument is True, 
plots the estimated results with the FSP method.</p>
<blockquote>
<div><ol class="arabic simple">
<li><p><strong>fsp_estimation</strong> (bool): If True, estimates the distribution with the FSP method. Defaults to False.</p></li>
<li><p><strong>stoich_mat</strong> (np.ndarray): Stoichiometry matrix.</p></li>
<li><p><strong>propensities</strong> (np.ndarray): Non-parameterised propensity functions.</p></li>
<li><p><strong>propensities_drv</strong> (np.ndarray): Gradient functions of the propensities with respect to the parameters.
Has shape <span class="math notranslate nohighlight">\((M, M_{\theta}+M_{\xi})\)</span>. If None, it is assumed that the Chemical Reaction Network follows mass-action kinetics.</p></li>
<li><p><span class="math notranslate nohighlight">\(C_r\)</span>: Value such that the projection of <span class="math notranslate nohighlight">\((0, ..., 0, C_r)\)</span> is the last element of the projected truncated space.</p></li>
<li><p><strong>init_state</strong> (np.ndarray): Initial state.</p></li>
<li><p><strong>ind_species</strong> (int): Index of the species of interest.</p></li>
<li><p><strong>n_fixed_params</strong> (int): Number of fixed parameters required to define the propensity functions <span class="math notranslate nohighlight">\(M_{\theta}\)</span>.</p></li>
<li><p><strong>n_control_params</strong> (int): Number of control parameters required to define the propensity functions <span class="math notranslate nohighlight">\(M_{\xi}\)</span>.
Their values vary from one time window to another.</p></li>
</ol>
</div></blockquote>
</li>
<li><p><strong>plot</strong> (Tuple[str, int], optional): The first argument is either “probabilities” to plot a probability distribution, or “sensitivities” 
to plot a sensitivity of the likelihood distribution. If it is “sensitivities”, second argument is the index of the parameter 
such that it plots the sensitivities with respect to this parameter. Defaults to (“probabilities”, None).</p></li>
<li><p><strong>n_col</strong> (int, optional): Number of columns to plot. Defaults to <span class="math notranslate nohighlight">\(2\)</span>.</p></li>
<li><p><strong>save</strong> (Tuple[bool, str], optional): If the first argument is True, saves the file. Second element is the name of the file 
in which to save the plot. Defaults to (False, None).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="plotting-fisher-information-results">
<h2>Plotting Fisher Information results<a class="headerlink" href="#plotting-fisher-information-results" title="Permalink to this heading"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="plot.fi_table">
<span class="sig-prename descclassname"><span class="pre">plot.</span></span><span class="sig-name descname"><span class="pre">fi_table</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">time_samples</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">params</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ind_param</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_windows</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">models</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(False,</span> <span class="pre">None,</span> <span class="pre">4)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">plot_exact_result</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(False,</span> <span class="pre">None)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">plot_fsp_result</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(False,</span> <span class="pre">None)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">up_bound</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">200</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_of_bounds_index</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(False,</span> <span class="pre">None)</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/plot.html#fi_table"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#plot.fi_table" title="Permalink to this definition"></a></dt>
<dd><p>Plots a table of the diagonal element of the Fisher Information estimated by various methods at various times.</p>
<dl>
<dt>Args:</dt><dd><ul>
<li><p><strong>time_samples</strong> (list): Sampling time.</p></li>
<li><p><strong>params</strong> (list): Parameters of the propensity functions in the form requested by the Mixture Density Networks:
<span class="math notranslate nohighlight">\([t, \theta_1, ..., \theta_{M_{\theta}}, \xi_1^1, \xi_1^2, ..., \xi_1^{M_{\xi}}, \xi_2^1, ..., \xi_L^{M_{\xi}}]\)</span>.</p></li>
<li><p><strong>ind_param</strong> (int): Index of the estimated Fisher Information diagonal value.</p></li>
<li><p><strong>time_windows</strong> (np.ndarray): Time windows during which the parameters do not vary. Its form is <span class="math notranslate nohighlight">\([t_1, ..., t_L]\)</span>,
such that the considered time windows are <span class="math notranslate nohighlight">\([0, t_1], [t_1, t_2], ..., [t_{L-1}, t_L]\)</span>. <span class="math notranslate nohighlight">\(t_L\)</span> must match
with the final time <span class="math notranslate nohighlight">\(t_f\)</span>. If there is only one time window, <strong>time_windows</strong> should be defined as <span class="math notranslate nohighlight">\([t_f]\)</span>.</p></li>
<li><p><strong>models</strong> (Tuple[bool, list, int], optional): Arguments to estimate the Fisher Information 
with MDN models. Defaults to (False, None, 4).</p>
<blockquote>
<div><ol class="arabic simple">
<li><p><strong>model_estimation</strong> (bool): If True, estimates the Fisher Information with MDN models.</p></li>
<li><p><strong>models_list</strong> (list): List of MDN models to use.</p></li>
<li><p><strong>n_comps</strong> (int): Number of mixture components.</p></li>
</ol>
</div></blockquote>
</li>
<li><p><strong>plot_exact_result</strong> (Tuple[bool, Callable], optional): Arguments to compute the exact value of the Fisher Information. 
Defaults to (False, None).</p>
<blockquote>
<div><ol class="arabic simple">
<li><p><strong>exact_value</strong> (bool): If True, computes the exact value of the Fisher Information.</p></li>
<li><p><strong>fisher_information_function</strong> (Callable): Exact function of the Fisher Information value.</p></li>
</ol>
</div></blockquote>
</li>
<li><p><strong>plot_fsp_result</strong> (Tuple[bool, np.ndarray, np.ndarray, int, np.ndarray, int, int, int], optional): Arguments to estimate the Fisher Information 
with the FSP method.</p>
<blockquote>
<div><ol class="arabic simple">
<li><p><strong>fsp_estimation</strong> (bool): If True, estimates the Fisher Information with the FSP method. Defaults to False.</p></li>
<li><p><strong>stoich_mat</strong> (np.ndarray): Stoichiometry matrix.</p></li>
<li><p><strong>propensities</strong> (np.ndarray): Non-parameterised propensity functions.</p></li>
<li><p><strong>propensities_drv</strong> (np.ndarray): Gradient functions of the propensities with respect to the parameters.
Has shape <span class="math notranslate nohighlight">\((M, M_{\theta}+M_{\xi})\)</span>. If None, it is assumed that the Chemical Reaction Network follows mass-action kinetics.</p></li>
<li><p><span class="math notranslate nohighlight">\(C_r\)</span>: Value such that the projection of <span class="math notranslate nohighlight">\((0, ..., 0, C_r)\)</span> is the last element of the projected truncated space.</p></li>
<li><p><strong>init_state</strong> (np.ndarray): Initial state.</p></li>
<li><p><strong>ind_species</strong> (int): Index of the species of interest.</p></li>
<li><p><strong>n_fixed_params</strong> (int): Number of fixed parameters required to define the propensity functions <span class="math notranslate nohighlight">\(M_{\theta}\)</span>.</p></li>
<li><p><strong>n_control_params</strong> (int): Number of control parameters required to define the propensity functions <span class="math notranslate nohighlight">\(M_{\xi}\)</span>.
Their values vary from one time window to another.</p></li>
</ol>
</div></blockquote>
</li>
<li><p><strong>up_bound</strong> (int, optional): Upper boundary of the predicted distribution. Defaults to <span class="math notranslate nohighlight">\(200\)</span>.</p></li>
<li><p><strong>out_of_bounds_index</strong> (int, optional): Index of the first time out of the training range in <strong>time_samples</strong>. If None, all
times are within the training range. Defaults to None.</p></li>
<li><p><strong>save</strong> (Tuple[bool, str], optional): If the first argument is True, saves the file. 
The second argument is the name of the file under which to save the plot. Defaults to (False, None).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="plot.fi_barplots">
<span class="sig-prename descclassname"><span class="pre">plot.</span></span><span class="sig-name descname"><span class="pre">fi_barplots</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">time_samples</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">params</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ind_param</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_windows</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">models</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(False,</span> <span class="pre">None,</span> <span class="pre">4)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">plot_exact_result</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(False,</span> <span class="pre">None)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">plot_fsp_result</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(False,</span> <span class="pre">None)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">up_bound</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">200</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(False,</span> <span class="pre">None)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">colors</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">['blue',</span> <span class="pre">'darkorange',</span> <span class="pre">'forestgreen']</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mean</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/plot.html#fi_barplots"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#plot.fi_barplots" title="Permalink to this definition"></a></dt>
<dd><p>Plots rectangular bars to visualize the diagonal element of the Fisher Information estimated by various methods at various times.</p>
<dl>
<dt>Args:</dt><dd><ul>
<li><p><strong>time_samples</strong> (np.ndarray): Sampling times.</p></li>
<li><p><strong>params</strong> (np.ndarray): Parameters of the propensity functions in the form requested by the Mixture Density Networks:
<span class="math notranslate nohighlight">\([t, \theta_1, ..., \theta_{M_{\theta}}, \xi_1^1, \xi_1^2, ..., \xi_1^{M_{\xi}}, \xi_2^1, ..., \xi_L^{M_{\xi}}]\)</span>.</p></li>
<li><p><strong>ind_param</strong> (int): Index of the estimated Fisher Information diagonal value.</p></li>
<li><p><strong>time_windows</strong> (np.ndarray): Time windows during which the parameters do not vary. Its form is <span class="math notranslate nohighlight">\([t_1, ..., t_L]\)</span>,
such that the considered time windows are <span class="math notranslate nohighlight">\([0, t_1], [t_1, t_2], ..., [t_{L-1}, t_L]\)</span>. <span class="math notranslate nohighlight">\(t_L\)</span> must match
with the final time <span class="math notranslate nohighlight">\(t_f\)</span>. If there is only one time window, <strong>time_windows</strong> should be defined as <span class="math notranslate nohighlight">\([t_f]\)</span>.</p></li>
<li><p><strong>models</strong> (Tuple[bool, list, int], optional): Arguments to estimate the Fisher Information 
with MDN models. Defaults to (False, None, 4).</p>
<blockquote>
<div><ol class="arabic simple">
<li><p><strong>model_estimation</strong> (bool): If True, estimates the Fisher Information with MDN models.</p></li>
<li><p><strong>models_list</strong> (list): List of MDN models from which to estimate the Fisher Information.</p></li>
<li><p><strong>n_comps</strong> (int): Number of mixture components.</p></li>
</ol>
</div></blockquote>
</li>
<li><p><strong>plot_exact_result</strong> (Tuple[bool, Callable], optional): Arguments to compute the exact value of the Fisher Information. 
Defaults to (False, None).</p>
<blockquote>
<div><ol class="arabic simple">
<li><p><strong>exact_value</strong> (bool): If True, computes the exact value of the Fisher Information.</p></li>
<li><p><strong>fisher_information_function</strong> (Callable): Exact function of the Fisher Information exact value.</p></li>
</ol>
</div></blockquote>
</li>
<li><p><strong>plot_fsp_result</strong> (Tuple[bool, np.ndarray, np.ndarray, int, np.ndarray, int, int, int], optional): Arguments to estimate the Fisher Information 
with the FSP method.</p>
<blockquote>
<div><ol class="arabic simple">
<li><p><strong>fsp_estimation</strong> (bool): If True, estimates the Fisher Information with the FSP method. Defaults to False.</p></li>
<li><p><strong>stoich_mat</strong> (np.ndarray): Stoichiometry matrix.</p></li>
<li><p><strong>propensities</strong> (np.ndarray): Non-parameterised propensity functions.</p></li>
<li><p><strong>propensities_drv</strong> (np.ndarray): Gradient functions of the propensities with respect to the parameters.
Has shape <span class="math notranslate nohighlight">\((M, M_{\theta}+M_{\xi})\)</span>. If None, it is assumed that the Chemical Reaction Network follows mass-action kinetics.</p></li>
<li><p><span class="math notranslate nohighlight">\(C_r\)</span>: Value such that the projection of <span class="math notranslate nohighlight">\((0, ..., 0, C_r)\)</span> is the last element of the projected truncated space.</p></li>
<li><p><strong>init_state</strong> (Tuple[int], optional): Initial state. If None, the initial state is set to <span class="math notranslate nohighlight">\((0,..,0)\)</span>.</p></li>
<li><p><strong>ind_species</strong> (int): Index of the species of interest.</p></li>
<li><p><strong>n_fixed_params</strong> (int): Number of fixed parameters required to define the propensity functions <span class="math notranslate nohighlight">\(M_{\theta}\)</span>.</p></li>
<li><p><strong>n_control_params</strong> (int): Number of control parameters required to define the propensity functions <span class="math notranslate nohighlight">\(M_{\xi}\)</span>.
Their values vary from one time window to another.</p></li>
</ol>
</div></blockquote>
</li>
<li><p><strong>up_bound</strong> (int, optional): Upper boundary of the predicted distribution. Defaults to <span class="math notranslate nohighlight">\(200\)</span>.</p></li>
<li><p><strong>save</strong> (Tuple[bool, str], optional): If the first argument is True, saves the file. 
The second argument is the name of the file under which to save the plot. Defaults to (False, None).</p></li>
<li><p><strong>colors</strong> (list, optional): Chosen colors for the bars. Defaults to [“blue”, “darkorange”, “forestgreen”].</p></li>
<li><p><strong>mean</strong> (bool, optional): Indicates whether to compute the mean of the MDN values or to plot a bar for each MDN value. Defaults to True.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="plotting-expectation-results">
<h2>Plotting expectation results<a class="headerlink" href="#plotting-expectation-results" title="Permalink to this heading"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="plot.expect_val_table">
<span class="sig-prename descclassname"><span class="pre">plot.</span></span><span class="sig-name descname"><span class="pre">expect_val_table</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">time_samples</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">params</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_windows</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">models</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(False,</span> <span class="pre">None,</span> <span class="pre">4)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">plot_exact_result</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(False,</span> <span class="pre">None)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">plot_fsp_result</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(False,</span> <span class="pre">None)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">up_bound</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">200</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">plot</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">('value',</span> <span class="pre">None)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_of_bounds_index</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(False,</span> <span class="pre">None)</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/plot.html#expect_val_table"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#plot.expect_val_table" title="Permalink to this definition"></a></dt>
<dd><p>Plots a table of the expectation <span class="math notranslate nohighlight">\(E_{\theta, \xi}[X_t]\)</span> or its gradient with respect to a specified parameter
<span class="math notranslate nohighlight">\(\frac{\partial E_{\theta, \xi}[X_t]}{\partial \theta_i}\)</span> or <span class="math notranslate nohighlight">\(\frac{\partial E_{\theta, \xi}[X_t]}{\partial \xi_i}\)</span>, 
estimated by various methods at various times.</p>
<dl>
<dt>Args:</dt><dd><ul>
<li><p><strong>time_samples</strong> (np.ndarray): Sampling times.</p></li>
<li><p><strong>params</strong> (np.ndarray): Parameters of the propensity functions in the form requested by the Mixture Density Networks:
<span class="math notranslate nohighlight">\([t, \theta_1, ..., \theta_{M_{\theta}}, \xi_1^1, \xi_1^2, ..., \xi_1^{M_{\xi}}, \xi_2^1, ..., \xi_L^{M_{\xi}}]\)</span>.</p></li>
<li><p><strong>time_windows</strong> (np.ndarray): Time windows during which the parameters do not vary. Its form is <span class="math notranslate nohighlight">\([t_1, ..., t_L]\)</span>,
such that the considered time windows are <span class="math notranslate nohighlight">\([0, t_1], [t_1, t_2], ..., [t_{L-1}, t_L]\)</span>. <span class="math notranslate nohighlight">\(t_L\)</span> must match
with the final time <span class="math notranslate nohighlight">\(t_f\)</span>. If there is only one time window, <strong>time_windows</strong> should be defined as <span class="math notranslate nohighlight">\([t_f]\)</span>.</p></li>
<li><p><strong>models</strong> (Tuple[bool, list, int], optional): Arguments to estimate the expected value with MDN models. 
Defaults to (False, None, 4).</p>
<blockquote>
<div><ol class="arabic simple">
<li><p><strong>model_estimation</strong> (bool): If True, estimates the expected value with MDN models.</p></li>
<li><p><strong>models_list</strong> (list): List of MDN models from which to estimate the expected value.</p></li>
<li><p><strong>n_comps</strong> (int): Number of mixture components.</p></li>
</ol>
</div></blockquote>
</li>
<li><p><strong>plot_exact_result</strong> (Tuple[bool, Callable], optional): Arguments to calculate the exact value of the Fisher Information. Defaults to (False, None).</p>
<blockquote>
<div><ol class="arabic simple">
<li><p><strong>exact_value</strong> (bool): If True, calculates the exact expected value.</p></li>
<li><p><strong>expected_value_function</strong> (Callable): Function that computes the expected value.</p></li>
</ol>
</div></blockquote>
</li>
<li><p><strong>plot_fsp_result</strong> (Tuple[bool, np.ndarray, np.ndarray, int, np.ndarray, int, int, int], optional): Arguments to estimate the Fisher Information 
with the FSP method.</p>
<blockquote>
<div><ol class="arabic simple">
<li><p><strong>fsp_estimation</strong> (bool): If True, estimates the expected value with the FSP method. Defaults to False.</p></li>
<li><p><strong>stoich_mat</strong> (np.ndarray): Stoichiometry matrix.</p></li>
<li><p><strong>propensities</strong> (np.ndarray[Callable]): Non-parameterised propensity functions.</p></li>
<li><p><strong>propensities_drv</strong> (np.ndarray): Gradient functions of the propensities with respect to the parameters.
Has shape <span class="math notranslate nohighlight">\((M, M_{\theta}+M_{\xi})\)</span>. If None, it is assumed that the Chemical Reaction Network follows mass-action kinetics.</p></li>
<li><p><span class="math notranslate nohighlight">\(C_r\)</span>: Value such that the projection of <span class="math notranslate nohighlight">\((0, ..., 0, C_r)\)</span> is the last element of the projected truncated space.</p></li>
<li><p><strong>init_state</strong> (Tuple[int], optional): Initial state. If None, the initial state is set to <span class="math notranslate nohighlight">\((0,..,0)\)</span>.</p></li>
<li><p><strong>ind_species</strong> (int): Index of the species of interest.</p></li>
<li><p><strong>n_fixed_params</strong> (int): Number of fixed parameters required to define the propensity functions <span class="math notranslate nohighlight">\(M_{\theta}\)</span>.</p></li>
<li><p><strong>n_control_params</strong> (int): Number of control parameters required to define the propensity functions <span class="math notranslate nohighlight">\(M_{\xi}\)</span>.
Their values vary from a time window to another.</p></li>
</ol>
</div></blockquote>
</li>
<li><p><strong>up_bound</strong> (int, optional): Upper boundary of the predicted distribution. Defaults to <span class="math notranslate nohighlight">\(200\)</span>.</p></li>
<li><p><strong>plot</strong> (Tuple[str, int], optional): The first argument is either “value” to compute the expected value, or “gradient” to compute the
gradient of the expected value. If it is “gradient”, the second argument is the index of the parameter such that it computes the gradient
with respect to this parameter. Defaults to (“value”, None).</p></li>
<li><p><strong>out_of_bounds_index</strong> (int, optional): Index of the first time out of the training range in <strong>time_samples</strong>. If None, all
times are within the training range. Defaults to None.</p></li>
<li><p><strong>save</strong> (Tuple[bool, str], optional): If the first argument is True, saves the file. 
The second argument is the name of the file under which to save the plot. Defaults to (False, None).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="plot.expect_val_barplots">
<span class="sig-prename descclassname"><span class="pre">plot.</span></span><span class="sig-name descname"><span class="pre">expect_val_barplots</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">time_samples</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">params</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_windows</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">models</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(False,</span> <span class="pre">None,</span> <span class="pre">4)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">plot_exact_result</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(False,</span> <span class="pre">None)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">plot_fsp_result</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(False,</span> <span class="pre">None)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">up_bound</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">200</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">plot</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">('value',</span> <span class="pre">None)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(False,</span> <span class="pre">None)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">colors</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">['blue',</span> <span class="pre">'darkorange',</span> <span class="pre">'forestgreen']</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mean</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/plot.html#expect_val_barplots"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#plot.expect_val_barplots" title="Permalink to this definition"></a></dt>
<dd><p>Plots rectangular bars to visualize the expectation <span class="math notranslate nohighlight">\(E_{\theta, \xi}[X_t]\)</span> or its gradient with respect to a specified parameter
<span class="math notranslate nohighlight">\(\frac{\partial E_{\theta, \xi}[X_t]}{\partial \theta_i}\)</span> or <span class="math notranslate nohighlight">\(\frac{\partial E_{\theta, \xi}[X_t]}{\partial \xi_i}\)</span>, 
estimated by various methods at various times.</p>
<dl>
<dt>Args:</dt><dd><ul>
<li><p><strong>time_samples</strong> (np.ndarray): Sampling times.</p></li>
<li><p><strong>params</strong> (np.ndarray): Parameters of the propensity functions in the form requested by the Mixture Density Networks:
<span class="math notranslate nohighlight">\([t, \theta_1, ..., \theta_{M_{\theta}}, \xi_1^1, \xi_1^2, ..., \xi_1^{M_{\xi}}, \xi_2^1, ..., \xi_L^{M_{\xi}}]\)</span>.</p></li>
<li><p><strong>time_windows</strong> (np.ndarray): Time windows during which the parameters do not vary. Its form is <span class="math notranslate nohighlight">\([t_1, ..., t_L]\)</span>,
such that the considered time windows are <span class="math notranslate nohighlight">\([0, t_1], [t_1, t_2], ..., [t_{L-1}, t_L]\)</span>. <span class="math notranslate nohighlight">\(t_L\)</span> must match
with the final time <span class="math notranslate nohighlight">\(t_f\)</span>. If there is only one time window, <strong>time_windows</strong> should be defined as <span class="math notranslate nohighlight">\([t_f]\)</span>.</p></li>
<li><p><strong>models</strong> (Tuple[bool, list, int], optional): Arguments to estimate the expected value with MDN models. 
Defaults to (False, None, 4).</p>
<blockquote>
<div><ol class="arabic simple">
<li><p><strong>model_estimation</strong> (bool): If True, estimates the expected value with MDN models.</p></li>
<li><p><strong>models_list</strong> (list): List of MDN models from which to estimate the expected value.</p></li>
<li><p><strong>n_comps</strong> (int): Number of mixture components.</p></li>
</ol>
</div></blockquote>
</li>
<li><p><strong>plot_exact_result</strong> (Tuple[bool, Callable], optional): Arguments to calculate the exact value of the Fisher Information. Defaults to (False, None).</p>
<blockquote>
<div><ol class="arabic simple">
<li><p><strong>exact_value</strong> (bool): If True, calculates the exact expected value.</p></li>
<li><p><strong>expected_value_function</strong> (Callable): Function that computes the expected value.</p></li>
</ol>
</div></blockquote>
</li>
<li><p><strong>plot_fsp_result</strong> (Tuple[bool, np.ndarray, np.ndarray, int, np.ndarray, int, int, int], optional): Arguments to estimate the Fisher Information 
with the FSP method.</p>
<blockquote>
<div><ol class="arabic simple">
<li><p><strong>fsp_estimation</strong> (bool): If True, estimates the expected value with the FSP method. Defaults to False.</p></li>
<li><p><strong>stoich_mat</strong> (np.ndarray): Stoichiometry matrix.</p></li>
<li><p><strong>propensities</strong> (np.ndarray[Callable]): Non-parameterised propensity functions.</p></li>
<li><p><strong>propensities_drv</strong> (np.ndarray): Gradient functions of the propensities with respect to the parameters.
Has shape <span class="math notranslate nohighlight">\((M, M_{\theta}+M_{\xi})\)</span>. If None, it is assumed that the Chemical Reaction Network follows mass-action kinetics.</p></li>
<li><p><span class="math notranslate nohighlight">\(C_r\)</span>: Value such that the projection of <span class="math notranslate nohighlight">\((0, ..., 0, C_r)\)</span> is the last element of the projected truncated space.</p></li>
<li><p><strong>init_state</strong> (Tuple[int], optional): Initial state. If None, the initial state is set to <span class="math notranslate nohighlight">\((0,..,0)\)</span>.</p></li>
<li><p><strong>ind_species</strong> (int): Index of the species of interest.</p></li>
<li><p><strong>n_fixed_params</strong> (int): Number of fixed parameters required to define the propensity functions <span class="math notranslate nohighlight">\(M_{\theta}\)</span>.</p></li>
<li><p><strong>n_control_params</strong> (int): Number of control parameters required to define the propensity functions <span class="math notranslate nohighlight">\(M_{\xi}\)</span>.
Their values vary from a time window to another.</p></li>
</ol>
</div></blockquote>
</li>
<li><p><strong>up_bound</strong> (int, optional): Upper boundary of the predicted distribution. Defaults to <span class="math notranslate nohighlight">\(200\)</span>.</p></li>
<li><p><strong>plot</strong> (Tuple[str, int], optional): The first argument is either “value” to compute the expected value, or “gradient” to compute the
gradient of the expected value. If it is “gradient”, the second argument is the index of the parameter such that it computes the gradient
with respect to this parameter. Defaults to (“value”, None).</p></li>
<li><p><strong>save</strong> (Tuple[bool, str], optional): If the first argument is True, saves the file. 
The second argument is the name of the file under which to save the plot. Defaults to (False, None).</p></li>
<li><p><strong>colors</strong> (list, optional): Chosen colors for the bars. Defaults to [“blue”, “darkorange”, “forestgreen”].</p></li>
<li><p><strong>mean</strong> (bool, optional): Indicates whether to compute the mean of the MDN values or to plot a bar for each MDN value. Defaults to True.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="saving-and-loading-a-mixture-density-network">
<h2>Saving and loading a Mixture Density Network<a class="headerlink" href="#saving-and-loading-a-mixture-density-network" title="Permalink to this heading"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="save_load_MDN.save_MDN_model">
<span class="sig-prename descclassname"><span class="pre">save_load_MDN.</span></span><span class="sig-name descname"><span class="pre">save_MDN_model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">MDNmodel</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">file_path</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/save_load_MDN.html#save_MDN_model"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#save_load_MDN.save_MDN_model" title="Permalink to this definition"></a></dt>
<dd><p>Saves the parameters of a Mixture Density Network model in a <cite>.pt</cite> file under <strong>file_path</strong>,
including weights, architecture, number of parameters in input, number of components in the output mixture
and the type of output mixture.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>MDNmodel</strong> (neuralnetwork.NeuralNetwork): MDN model to save.</p></li>
<li><p><strong>file_path</strong> (str): Path of the file in which to save the parameters.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="save_load_MDN.load_MDN_model">
<span class="sig-prename descclassname"><span class="pre">save_load_MDN.</span></span><span class="sig-name descname"><span class="pre">load_MDN_model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_path</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/save_load_MDN.html#load_MDN_model"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#save_load_MDN.load_MDN_model" title="Permalink to this definition"></a></dt>
<dd><p>Loads a Mixture Density Network model from a <cite>.pt</cite> file.</p>
<dl class="simple">
<dt>Args:</dt><dd><ul class="simple">
<li><p><strong>file_path</strong> (str): Path of the file in which the parameters of the model are saved.</p></li>
</ul>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>Loaded MDN model.</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#neuralnetwork.NeuralNetwork" title="neuralnetwork.NeuralNetwork"><code class="xref py py-class docutils literal notranslate"><span class="pre">NeuralNetwork</span></code></a></p>
</dd>
</dl>
</dd></dl>

</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="advice.html" class="btn btn-neutral float-left" title="Some advice on the implementation of the approach" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="neuralnetwork.html" class="btn btn-neutral float-right" title="Background on the Architecture of the Mixture Density Network" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Control Theory and Systems Biology Laboratory, D-BSSE, ETH Zurich.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>